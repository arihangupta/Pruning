{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3470db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"/Users/arihangupta/.medmnist/pathmnist.npz\"\n",
    "data = np.load(file_path)\n",
    "\n",
    "images = data[\"train_images\"]\n",
    "labels = data[\"train_labels\"]\n",
    "\n",
    "# Prepare figure\n",
    "fig, ax = plt.subplots()\n",
    "index = 0\n",
    "\n",
    "def show_image(idx):\n",
    "    ax.clear()\n",
    "    img = images[idx]\n",
    "    if img.ndim == 2:\n",
    "        ax.imshow(img, cmap=\"gray\")\n",
    "    else:\n",
    "        ax.imshow(img)\n",
    "    ax.set_title(f\"Index: {idx} | Label: {labels[idx][0]}\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.canvas.draw()\n",
    "\n",
    "# Initial display\n",
    "show_image(index)\n",
    "\n",
    "def on_key(event):\n",
    "    global index\n",
    "    if event.key == \"right\":\n",
    "        index = (index + 1) % len(images)  # next image\n",
    "    elif event.key == \"left\":\n",
    "        index = (index - 1) % len(images)  # previous image\n",
    "    show_image(index)\n",
    "\n",
    "# Connect arrow keys\n",
    "fig.canvas.mpl_connect(\"key_press_event\", on_key)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dfb3da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 24ms/step - accuracy: 0.6054 - loss: 1.0542 - val_accuracy: 0.6836 - val_loss: 0.8660\n",
      "Epoch 2/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 26ms/step - accuracy: 0.7312 - loss: 0.7310 - val_accuracy: 0.7175 - val_loss: 0.7945\n",
      "Epoch 3/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 28ms/step - accuracy: 0.7745 - loss: 0.6189 - val_accuracy: 0.7684 - val_loss: 0.6426\n",
      "Epoch 4/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 29ms/step - accuracy: 0.8000 - loss: 0.5467 - val_accuracy: 0.7706 - val_loss: 0.6438\n",
      "Epoch 5/5\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 26ms/step - accuracy: 0.8220 - loss: 0.4857 - val_accuracy: 0.8208 - val_loss: 0.4824\n",
      "225/225 - 1s - 5ms/step - accuracy: 0.7460 - loss: 0.7622\n",
      "\n",
      "✅ Test Accuracy: 0.7460\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# -----------------\n",
    "# Load MedMNIST subset (e.g. PathMNIST)\n",
    "# -----------------\n",
    "file_path = \"/Users/arihangupta/.medmnist/pathmnist.npz\"\n",
    "data = np.load(file_path)\n",
    "\n",
    "X_train, y_train = data[\"train_images\"], data[\"train_labels\"].flatten()\n",
    "X_val, y_val     = data[\"val_images\"], data[\"val_labels\"].flatten()\n",
    "X_test, y_test   = data[\"test_images\"], data[\"test_labels\"].flatten()\n",
    "\n",
    "# Normalize\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_val   = X_val.astype(\"float32\") / 255.0\n",
    "X_test  = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# -----------------\n",
    "# Build CNN model\n",
    "# -----------------\n",
    "def build_model(input_shape, num_classes, learning_rate=0.001):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.Conv2D(64, (3,3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.Conv2D(128, (3,3), activation=\"relu\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# -----------------\n",
    "# Training function\n",
    "# -----------------\n",
    "def train_model(epochs=10, batch_size=32, learning_rate=0.001):\n",
    "    input_shape = X_train.shape[1:]  # (height, width, channels)\n",
    "    model = build_model(input_shape, num_classes, learning_rate)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(f\"\\n✅ Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# -----------------\n",
    "# Example usage\n",
    "# -----------------\n",
    "# Control parameters here\n",
    "model, history = train_model(\n",
    "    epochs=5,         # change epochs\n",
    "    batch_size=64,    # change batch size\n",
    "    learning_rate=1e-3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f7a0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "Epoch 1/3\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2982s\u001b[0m 1s/step - accuracy: 0.1692 - loss: 2.1729 - val_accuracy: 0.2137 - val_loss: 2.1123\n",
      "Epoch 2/3\n",
      "\u001b[1m1224/2813\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m24:35\u001b[0m 929ms/step - accuracy: 0.2127 - loss: 2.1092"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 96\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# -----------------\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# -----------------\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m model, history \u001b[38;5;241m=\u001b[39m train_resnet(\n\u001b[1;32m     97\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     98\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     99\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m\n\u001b[1;32m    100\u001b[0m )\n",
      "Cell \u001b[0;32mIn[1], line 81\u001b[0m, in \u001b[0;36mtrain_resnet\u001b[0;34m(epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     78\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (IMG_SIZE, IMG_SIZE, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     79\u001b[0m model \u001b[38;5;241m=\u001b[39m build_resnet(input_shape, num_classes, learning_rate)\n\u001b[0;32m---> 81\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     82\u001b[0m     train_ds,\n\u001b[1;32m     83\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mval_ds,\n\u001b[1;32m     84\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m     85\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     88\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_ds, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🎯 Final Test Accuracy (ResNet50): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    376\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[0;32m--> 377\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    378\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    218\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    219\u001b[0m     ):\n\u001b[0;32m--> 220\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1506\u001b[0m   )\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# -----------------\n",
    "# Load MedMNIST subset (PathMNIST example)\n",
    "# -----------------\n",
    "file_path = \"/Users/arihangupta/.medmnist/pathmnist.npz\"\n",
    "data = np.load(file_path)\n",
    "\n",
    "X_train, y_train = data[\"train_images\"], data[\"train_labels\"].flatten()\n",
    "X_val, y_val     = data[\"val_images\"], data[\"val_labels\"].flatten()\n",
    "X_test, y_test   = data[\"test_images\"], data[\"test_labels\"].flatten()\n",
    "\n",
    "# -----------------\n",
    "# Preprocess (normalize, expand to 3 channels if grayscale)\n",
    "# -----------------\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_val   = X_val.astype(\"float32\") / 255.0\n",
    "X_test  = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "if X_train.ndim == 3 or X_train.shape[-1] == 1:\n",
    "    X_train = np.repeat(X_train[..., np.newaxis], 3, axis=-1)\n",
    "    X_val   = np.repeat(X_val[..., np.newaxis], 3, axis=-1)\n",
    "    X_test  = np.repeat(X_test[..., np.newaxis], 3, axis=-1)\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# -----------------\n",
    "# Data pipeline (resize on the fly instead of all at once)\n",
    "# -----------------\n",
    "IMG_SIZE = 224\n",
    "\n",
    "def make_dataset(images, labels, batch_size=32, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(images))\n",
    "    ds = ds.map(lambda x, y: (tf.image.resize(x, [IMG_SIZE, IMG_SIZE]), y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_dataset(X_train, y_train, batch_size=32, shuffle=True)\n",
    "val_ds   = make_dataset(X_val, y_val, batch_size=32, shuffle=False)\n",
    "test_ds  = make_dataset(X_test, y_test, batch_size=32, shuffle=False)\n",
    "\n",
    "# -----------------\n",
    "# Build ResNet50 model\n",
    "# -----------------\n",
    "def build_resnet(input_shape, num_classes, learning_rate=1e-4):\n",
    "    base_model = ResNet50(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    base_model.trainable = False  # freeze backbone\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# -----------------\n",
    "# Training function\n",
    "# -----------------\n",
    "def train_resnet(epochs=5, batch_size=32, learning_rate=1e-4):\n",
    "    input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "    model = build_resnet(input_shape, num_classes, learning_rate)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_ds, verbose=2)\n",
    "    print(f\"\\n🎯 Final Test Accuracy (ResNet50): {test_acc:.4f}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# -----------------\n",
    "# Example usage\n",
    "# -----------------\n",
    "model, history = train_resnet(\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63da2d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training base CNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1: loss=1.2787, val_loss=0.9539, acc=0.5220, val_acc=0.6373\n",
      "✅ Epoch 2: loss=0.8517, val_loss=0.6645, acc=0.6932, val_acc=0.7542\n",
      "✅ Epoch 3: loss=0.6919, val_loss=0.5030, acc=0.7537, val_acc=0.8132\n",
      "✅ Epoch 4: loss=0.5998, val_loss=0.4450, acc=0.7865, val_acc=0.8360\n",
      "✅ Epoch 5: loss=0.5211, val_loss=0.4386, acc=0.8149, val_acc=0.8398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Test Accuracy (Unpruned CNN): 0.7898\n",
      "💾 Saved unpruned model as saved_models/cnn_unpruned.h5\n",
      "\n",
      "✂️ Applying manual pruning (threshold=0.01)...\n",
      "🛠 Fine-tuning pruned model for 2 epochs...\n",
      "✅ Epoch 1: loss=0.4671, val_loss=0.4288, acc=0.8339, val_acc=0.8450\n",
      "✅ Epoch 2: loss=0.4282, val_loss=0.3771, acc=0.8475, val_acc=0.8692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Test Accuracy (Pruned CNN): 0.8070\n",
      "💾 Saved pruned model as saved_models/cnn_pruned.h5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "\n",
    "# -----------------\n",
    "# Load MedMNIST subset (PathMNIST example)\n",
    "# -----------------\n",
    "file_path = \"/Users/arihangupta/.medmnist/pathmnist.npz\"\n",
    "data = np.load(file_path)\n",
    "\n",
    "X_train, y_train = data[\"train_images\"], data[\"train_labels\"].flatten()\n",
    "X_val, y_val     = data[\"val_images\"], data[\"val_labels\"].flatten()\n",
    "X_test, y_test   = data[\"test_images\"], data[\"test_labels\"].flatten()\n",
    "\n",
    "# -----------------\n",
    "# Preprocess (normalize, expand grayscale → 3 channels if needed)\n",
    "# -----------------\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_val   = X_val.astype(\"float32\") / 255.0\n",
    "X_test  = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "if X_train.ndim == 3 or X_train.shape[-1] == 1:\n",
    "    X_train = np.repeat(X_train[..., np.newaxis], 3, axis=-1)\n",
    "    X_val   = np.repeat(X_val[..., np.newaxis], 3, axis=-1)\n",
    "    X_test  = np.repeat(X_test[..., np.newaxis], 3, axis=-1)\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# -----------------\n",
    "# Data pipeline (resize small → 64x64 for speed)\n",
    "# -----------------\n",
    "IMG_SIZE = 64\n",
    "\n",
    "def make_dataset(images, labels, batch_size=32, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(images))\n",
    "    ds = ds.map(lambda x, y: (tf.image.resize(x, [IMG_SIZE, IMG_SIZE]), y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_dataset(X_train, y_train, batch_size=32, shuffle=True)\n",
    "val_ds   = make_dataset(X_val, y_val, batch_size=32, shuffle=False)\n",
    "test_ds  = make_dataset(X_test, y_test, batch_size=32, shuffle=False)\n",
    "\n",
    "# -----------------\n",
    "# Build a lightweight CNN\n",
    "# -----------------\n",
    "def build_cnn(input_shape, num_classes, learning_rate=1e-3):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Conv2D(64, (3,3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Conv2D(128, (3,3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# -----------------\n",
    "# Training progress callback\n",
    "# -----------------\n",
    "class TrainingProgress(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"✅ Epoch {epoch+1}: \"\n",
    "              f\"loss={logs['loss']:.4f}, \"\n",
    "              f\"val_loss={logs['val_loss']:.4f}, \"\n",
    "              f\"acc={logs['accuracy']:.4f}, \"\n",
    "              f\"val_acc={logs['val_accuracy']:.4f}\")\n",
    "\n",
    "# -----------------\n",
    "# Manual unstructured pruning\n",
    "# -----------------\n",
    "def prune_weights(model, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Zero out weights with magnitude below threshold.\n",
    "    Applies to Conv2D and Dense layers.\n",
    "    \"\"\"\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            w[np.abs(w) < threshold] = 0\n",
    "            layer.set_weights([w, b])\n",
    "    return model\n",
    "\n",
    "# -----------------\n",
    "# Train, prune, save, evaluate\n",
    "# -----------------\n",
    "def train_and_prune(epochs=5, learning_rate=1e-3, pruning_threshold=0.01, fine_tune_epochs=2):\n",
    "    input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "    # --- Train base CNN ---\n",
    "    print(\"\\n🚀 Training base CNN...\")\n",
    "    model = build_cnn(input_shape, num_classes, learning_rate)\n",
    "    model.fit(train_ds, validation_data=val_ds,\n",
    "              epochs=epochs, callbacks=[TrainingProgress()], verbose=0)\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
    "    print(f\"\\n🎯 Test Accuracy (Unpruned CNN): {test_acc:.4f}\")\n",
    "\n",
    "    # Save base model\n",
    "    os.makedirs(\"saved_models\", exist_ok=True)\n",
    "    unpruned_path = \"saved_models/cnn_unpruned.h5\"\n",
    "    model.save(unpruned_path)\n",
    "    print(f\"💾 Saved unpruned model as {unpruned_path}\")\n",
    "\n",
    "    # --- Manual pruning ---\n",
    "    print(f\"\\n✂️ Applying manual pruning (threshold={pruning_threshold})...\")\n",
    "    pruned_model = prune_weights(model, threshold=pruning_threshold)\n",
    "\n",
    "    # Optional fine-tune after pruning\n",
    "    if fine_tune_epochs > 0:\n",
    "        print(f\"🛠 Fine-tuning pruned model for {fine_tune_epochs} epochs...\")\n",
    "        pruned_model.fit(train_ds, validation_data=val_ds,\n",
    "                         epochs=fine_tune_epochs, callbacks=[TrainingProgress()], verbose=0)\n",
    "\n",
    "    test_loss, test_acc = pruned_model.evaluate(test_ds, verbose=0)\n",
    "    print(f\"\\n🎯 Test Accuracy (Pruned CNN): {test_acc:.4f}\")\n",
    "\n",
    "    pruned_path = \"saved_models/cnn_pruned.h5\"\n",
    "    pruned_model.save(pruned_path)\n",
    "    print(f\"💾 Saved pruned model as {pruned_path}\")\n",
    "\n",
    "    return model, pruned_model\n",
    "\n",
    "# -----------------\n",
    "# Run\n",
    "# -----------------\n",
    "base_model, pruned_model = train_and_prune(\n",
    "    epochs=5,\n",
    "    learning_rate=1e-3,\n",
    "    pruning_threshold=0.01,\n",
    "    fine_tune_epochs=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a31c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Unpruned model size: 7.88 MB\n",
      "📦 Pruned model size:   7.88 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def print_model_sizes(unpruned_path, pruned_path):\n",
    "    size_unpruned = os.path.getsize(unpruned_path) / (1024*1024)\n",
    "    size_pruned   = os.path.getsize(pruned_path) / (1024*1024)\n",
    "    print(f\"📦 Unpruned model size: {size_unpruned:.2f} MB\")\n",
    "    print(f\"📦 Pruned model size:   {size_pruned:.2f} MB\")\n",
    "\n",
    "unpruned_path = \"saved_models/cnn_unpruned.h5\"\n",
    "pruned_path   = \"saved_models/cnn_pruned.h5\"\n",
    "\n",
    "print_model_sizes(unpruned_path, pruned_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9e29ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "\n",
      "Measuring inference speed on test set...\n",
      "⏱ Unpruned CNN inference time on test set: 3.0476 seconds\n",
      "⏱ Pruned CNN inference time on test set: 2.6050 seconds\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# -----------------\n",
    "# Paths to saved models\n",
    "# -----------------\n",
    "unpruned_path = \"saved_models/cnn_unpruned.h5\"\n",
    "pruned_path   = \"saved_models/cnn_pruned.h5\"\n",
    "\n",
    "# -----------------\n",
    "# Load test dataset\n",
    "# -----------------\n",
    "file_path = \"/Users/arihangupta/.medmnist/pathmnist.npz\"\n",
    "data = np.load(file_path)\n",
    "X_test, y_test = data[\"test_images\"], data[\"test_labels\"].flatten()\n",
    "\n",
    "# Normalize\n",
    "X_test = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Expand grayscale → 3 channels if needed\n",
    "if X_test.ndim == 3 or X_test.shape[-1] == 1:\n",
    "    X_test = np.repeat(X_test[..., np.newaxis], 3, axis=-1)\n",
    "\n",
    "# Resize to 64x64\n",
    "IMG_SIZE = 64\n",
    "X_test_resized = np.array([tf.image.resize(img, [IMG_SIZE, IMG_SIZE]).numpy() for img in X_test])\n",
    "\n",
    "# Prepare dataset\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test_resized, y_test)).batch(32)\n",
    "\n",
    "# -----------------\n",
    "# Load models\n",
    "# -----------------\n",
    "print(\"Loading models...\")\n",
    "unpruned_model = tf.keras.models.load_model(unpruned_path)\n",
    "pruned_model   = tf.keras.models.load_model(pruned_path)\n",
    "\n",
    "# -----------------\n",
    "# Measure inference speed\n",
    "# -----------------\n",
    "def measure_inference_speed(model, dataset, name=\"Model\"):\n",
    "    start = time.time()\n",
    "    _ = model.predict(dataset, verbose=0)\n",
    "    end = time.time()\n",
    "    print(f\"⏱ {name} inference time on test set: {end - start:.4f} seconds\")\n",
    "\n",
    "print(\"\\nMeasuring inference speed on test set...\")\n",
    "measure_inference_speed(unpruned_model, test_ds, name=\"Unpruned CNN\")\n",
    "measure_inference_speed(pruned_model, test_ds, name=\"Pruned CNN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5198cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Unpruned model: 0/684361 weights are zero (0.00%)\n",
      "📦 Pruned model:   123003/684361 weights are zero (17.97%)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Paths to saved models\n",
    "unpruned_path = \"saved_models/cnn_unpruned.h5\"\n",
    "pruned_path   = \"saved_models/cnn_pruned.h5\"\n",
    "\n",
    "# Load models\n",
    "unpruned_model = tf.keras.models.load_model(unpruned_path)\n",
    "pruned_model   = tf.keras.models.load_model(pruned_path)\n",
    "\n",
    "# Function to count zero weights\n",
    "def count_zero_weights(model):\n",
    "    total_weights = 0\n",
    "    zero_weights = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            total_weights += w.size + b.size\n",
    "            zero_weights += np.sum(w == 0) + np.sum(b == 0)\n",
    "    return zero_weights, total_weights\n",
    "\n",
    "# Count zeroed weights\n",
    "zero_unpruned, total_unpruned = count_zero_weights(unpruned_model)\n",
    "zero_pruned, total_pruned     = count_zero_weights(pruned_model)\n",
    "\n",
    "# Print results\n",
    "print(f\"📦 Unpruned model: {zero_unpruned}/{total_unpruned} weights are zero \"\n",
    "      f\"({zero_unpruned/total_unpruned*100:.2f}%)\")\n",
    "print(f\"📦 Pruned model:   {zero_pruned}/{total_pruned} weights are zero \"\n",
    "      f\"({zero_pruned/total_pruned*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b2c0565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training base CNN...\n",
      "✅ Epoch 1: loss=1.3195, val_loss=0.9182, acc=0.5063, val_acc=0.6601\n",
      "✅ Epoch 2: loss=0.8515, val_loss=0.6231, acc=0.6946, val_acc=0.7683\n",
      "✅ Epoch 3: loss=0.7196, val_loss=0.5551, acc=0.7429, val_acc=0.7963\n",
      "✅ Epoch 4: loss=0.6320, val_loss=0.5170, acc=0.7735, val_acc=0.8104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 5: loss=0.5578, val_loss=0.6971, acc=0.8020, val_acc=0.7641\n",
      "\n",
      "📦 Unpruned: zeros=0/684361 (0.00%), inference=2.5706s, accuracy=0.7662\n",
      "\n",
      "✂️ Pruning with threshold=0.001...\n",
      "✅ Epoch 1: loss=0.5673, val_loss=0.7358, acc=0.8075, val_acc=0.8031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2: loss=0.5667, val_loss=0.5041, acc=0.8117, val_acc=0.8218\n",
      "📦 Pruned (t=0.001): zeros=11609/684361 (1.70%), inference=2.4865s, accuracy=0.8025\n",
      "\n",
      "✂️ Pruning with threshold=0.01...\n",
      "✅ Epoch 1: loss=0.5655, val_loss=0.5940, acc=0.8076, val_acc=0.8129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2: loss=0.5649, val_loss=0.4954, acc=0.8137, val_acc=0.8357\n",
      "📦 Pruned (t=0.01): zeros=117688/684361 (17.20%), inference=3.3615s, accuracy=0.8031\n",
      "\n",
      "✂️ Pruning with threshold=0.05...\n",
      "✅ Epoch 1: loss=0.5640, val_loss=1.6300, acc=0.8081, val_acc=0.5784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2: loss=0.5672, val_loss=0.4670, acc=0.8108, val_acc=0.8394\n",
      "📦 Pruned (t=0.05): zeros=448979/684361 (65.61%), inference=2.6498s, accuracy=0.7965\n",
      "\n",
      "✂️ Pruning with threshold=0.1...\n",
      "✅ Epoch 1: loss=0.5861, val_loss=0.5993, acc=0.7988, val_acc=0.7973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2: loss=0.5855, val_loss=0.6784, acc=0.8051, val_acc=0.7730\n",
      "📦 Pruned (t=0.1): zeros=481513/684361 (70.36%), inference=2.4510s, accuracy=0.8102\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "import time\n",
    "\n",
    "# -----------------\n",
    "# Load MedMNIST subset (PathMNIST example)\n",
    "# -----------------\n",
    "file_path = \"/Users/arihangupta/.medmnist/pathmnist.npz\"\n",
    "data = np.load(file_path)\n",
    "\n",
    "X_train, y_train = data[\"train_images\"], data[\"train_labels\"].flatten()\n",
    "X_val, y_val     = data[\"val_images\"], data[\"val_labels\"].flatten()\n",
    "X_test, y_test   = data[\"test_images\"], data[\"test_labels\"].flatten()\n",
    "\n",
    "# -----------------\n",
    "# Preprocess\n",
    "# -----------------\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_val   = X_val.astype(\"float32\") / 255.0\n",
    "X_test  = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "if X_train.ndim == 3 or X_train.shape[-1] == 1:\n",
    "    X_train = np.repeat(X_train[..., np.newaxis], 3, axis=-1)\n",
    "    X_val   = np.repeat(X_val[..., np.newaxis], 3, axis=-1)\n",
    "    X_test  = np.repeat(X_test[..., np.newaxis], 3, axis=-1)\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# -----------------\n",
    "# Data pipeline (resize → 64x64)\n",
    "# -----------------\n",
    "IMG_SIZE = 64\n",
    "\n",
    "def make_dataset(images, labels, batch_size=32, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(images))\n",
    "    ds = ds.map(lambda x, y: (tf.image.resize(x, [IMG_SIZE, IMG_SIZE]), y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_dataset(X_train, y_train, batch_size=32, shuffle=True)\n",
    "val_ds   = make_dataset(X_val, y_val, batch_size=32, shuffle=False)\n",
    "test_ds  = make_dataset(X_test, y_test, batch_size=32, shuffle=False)\n",
    "\n",
    "# -----------------\n",
    "# Build CNN\n",
    "# -----------------\n",
    "def build_cnn(input_shape, num_classes, learning_rate=1e-3):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Conv2D(64, (3,3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Conv2D(128, (3,3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# -----------------\n",
    "# Training progress callback\n",
    "# -----------------\n",
    "class TrainingProgress(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"✅ Epoch {epoch+1}: \"\n",
    "              f\"loss={logs['loss']:.4f}, \"\n",
    "              f\"val_loss={logs['val_loss']:.4f}, \"\n",
    "              f\"acc={logs['accuracy']:.4f}, \"\n",
    "              f\"val_acc={logs['val_accuracy']:.4f}\")\n",
    "\n",
    "# -----------------\n",
    "# Manual unstructured pruning\n",
    "# -----------------\n",
    "def prune_weights(model, threshold=0.01):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            w[np.abs(w) < threshold] = 0\n",
    "            layer.set_weights([w, b])\n",
    "    return model\n",
    "\n",
    "# -----------------\n",
    "# Count zero weights\n",
    "# -----------------\n",
    "def count_zero_weights(model):\n",
    "    total_weights = 0\n",
    "    zero_weights = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            total_weights += w.size + b.size\n",
    "            zero_weights += np.sum(w == 0) + np.sum(b == 0)\n",
    "    return zero_weights, total_weights\n",
    "\n",
    "# -----------------\n",
    "# Measure inference time\n",
    "# -----------------\n",
    "def measure_inference_time(model, dataset):\n",
    "    start = time.time()\n",
    "    _ = model.predict(dataset, verbose=0)\n",
    "    end = time.time()\n",
    "    return end - start\n",
    "\n",
    "# -----------------\n",
    "# Main workflow\n",
    "# -----------------\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "\n",
    "# --- Train base CNN ---\n",
    "print(\"\\n🚀 Training base CNN...\")\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "base_model = build_cnn(input_shape, num_classes)\n",
    "base_model.fit(train_ds, validation_data=val_ds, epochs=5, callbacks=[TrainingProgress()], verbose=0)\n",
    "\n",
    "# Save unpruned\n",
    "unpruned_path = \"saved_models/cnn_unpruned.h5\"\n",
    "base_model.save(unpruned_path)\n",
    "\n",
    "zero, total = count_zero_weights(base_model)\n",
    "time_inf = measure_inference_time(base_model, test_ds)\n",
    "test_loss, test_acc = base_model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "print(f\"\\n📦 Unpruned: zeros={zero}/{total} ({zero/total*100:.2f}%), \"\n",
    "      f\"inference={time_inf:.4f}s, accuracy={test_acc:.4f}\")\n",
    "\n",
    "# --- Prune with multiple thresholds ---\n",
    "thresholds = [0.001, 0.01, 0.05, 0.1]\n",
    "pruned_models = {}\n",
    "\n",
    "for t in thresholds:\n",
    "    print(f\"\\n✂️ Pruning with threshold={t}...\")\n",
    "    # Clone base model\n",
    "    model_copy = tf.keras.models.clone_model(base_model)\n",
    "    model_copy.set_weights(base_model.get_weights())\n",
    "    model_copy = prune_weights(model_copy, threshold=t)\n",
    "    \n",
    "    # Fine-tune pruned model (optional, 2 epochs)\n",
    "    model_copy.fit(train_ds, validation_data=val_ds, epochs=2, callbacks=[TrainingProgress()], verbose=0)\n",
    "    \n",
    "    # Save pruned model\n",
    "    path = f\"saved_models/cnn_pruned_{t}.h5\"\n",
    "    model_copy.save(path)\n",
    "    pruned_models[t] = model_copy\n",
    "    \n",
    "    # Evaluate\n",
    "    zero, total = count_zero_weights(model_copy)\n",
    "    time_inf = measure_inference_time(model_copy, test_ds)\n",
    "    test_loss, test_acc = model_copy.evaluate(test_ds, verbose=0)\n",
    "    \n",
    "    print(f\"📦 Pruned (t={t}): zeros={zero}/{total} ({zero/total*100:.2f}%), \"\n",
    "          f\"inference={time_inf:.4f}s, accuracy={test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4736be76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training base CNN...\n",
      "✅ Epoch 1: loss=1.1958, val_loss=0.8516, acc=0.5513, val_acc=0.6797\n",
      "✅ Epoch 2: loss=0.8225, val_loss=0.6214, acc=0.7008, val_acc=0.7760\n",
      "✅ Epoch 3: loss=0.6740, val_loss=0.5108, acc=0.7567, val_acc=0.8067\n",
      "✅ Epoch 4: loss=0.5787, val_loss=0.4310, acc=0.7942, val_acc=0.8498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 5: loss=0.5141, val_loss=0.3885, acc=0.8179, val_acc=0.8590\n",
      "\n",
      "📦 Unpruned: zeros=0/684361 (0.00%), inference=2.6877s, accuracy=0.8238\n",
      "✅ Epoch 1: loss=0.5751, val_loss=0.5680, acc=0.8024, val_acc=0.7997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2: loss=0.5561, val_loss=0.5364, acc=0.8142, val_acc=0.8186\n",
      "✅ Epoch 1: loss=0.5407, val_loss=0.4848, acc=0.8178, val_acc=0.8333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2: loss=0.5562, val_loss=0.5721, acc=0.8183, val_acc=0.8226\n",
      "✅ Epoch 1: loss=0.5700, val_loss=0.4204, acc=0.8040, val_acc=0.8522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2: loss=0.5602, val_loss=0.5552, acc=0.8152, val_acc=0.8120\n",
      "\n",
      "✅ Pruning Comparison Summary:\n",
      "           Method  % Zero Weights  Test Accuracy  Inference Time (s)\n",
      "0        Unpruned        0.000000       0.823816            2.687696\n",
      "1      Manual 0.1       67.696435       0.776184            2.519635\n",
      "2      Global 70%       60.103659       0.782033            2.469875\n",
      "3  Layer-wise 70%       61.065140       0.818524            2.531883\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------\n",
    "# Load MedMNIST subset (PathMNIST example)\n",
    "# -----------------\n",
    "file_path = \"/Users/arihangupta/.medmnist/pathmnist.npz\"\n",
    "data = np.load(file_path)\n",
    "\n",
    "X_train, y_train = data[\"train_images\"], data[\"train_labels\"].flatten()\n",
    "X_val, y_val     = data[\"val_images\"], data[\"val_labels\"].flatten()\n",
    "X_test, y_test   = data[\"test_images\"], data[\"test_labels\"].flatten()\n",
    "\n",
    "# -----------------\n",
    "# Preprocess\n",
    "# -----------------\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_val   = X_val.astype(\"float32\") / 255.0\n",
    "X_test  = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "if X_train.ndim == 3 or X_train.shape[-1] == 1:\n",
    "    X_train = np.repeat(X_train[..., np.newaxis], 3, axis=-1)\n",
    "    X_val   = np.repeat(X_val[..., np.newaxis], 3, axis=-1)\n",
    "    X_test  = np.repeat(X_test[..., np.newaxis], 3, axis=-1)\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# -----------------\n",
    "# Data pipeline (resize → 64x64)\n",
    "# -----------------\n",
    "IMG_SIZE = 64\n",
    "\n",
    "def make_dataset(images, labels, batch_size=32, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(images))\n",
    "    ds = ds.map(lambda x, y: (tf.image.resize(x, [IMG_SIZE, IMG_SIZE]), y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_dataset(X_train, y_train, batch_size=32, shuffle=True)\n",
    "val_ds   = make_dataset(X_val, y_val, batch_size=32, shuffle=False)\n",
    "test_ds  = make_dataset(X_test, y_test, batch_size=32, shuffle=False)\n",
    "\n",
    "# -----------------\n",
    "# Build CNN\n",
    "# -----------------\n",
    "def build_cnn(input_shape, num_classes, learning_rate=1e-3):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Conv2D(64, (3,3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Conv2D(128, (3,3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# -----------------\n",
    "# Training progress callback\n",
    "# -----------------\n",
    "class TrainingProgress(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"✅ Epoch {epoch+1}: \"\n",
    "              f\"loss={logs['loss']:.4f}, \"\n",
    "              f\"val_loss={logs['val_loss']:.4f}, \"\n",
    "              f\"acc={logs['accuracy']:.4f}, \"\n",
    "              f\"val_acc={logs['val_accuracy']:.4f}\")\n",
    "\n",
    "# -----------------\n",
    "# Pruning functions\n",
    "# -----------------\n",
    "def manual_prune(model, threshold=0.1):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            w[np.abs(w) < threshold] = 0\n",
    "            layer.set_weights([w, b])\n",
    "    return model\n",
    "\n",
    "def global_prune(model, sparsity=0.7):\n",
    "    # Collect all weights\n",
    "    all_weights = []\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, _ = layer.get_weights()\n",
    "            all_weights.extend(np.abs(w).flatten())\n",
    "    threshold = np.percentile(all_weights, sparsity*100)\n",
    "    # Apply pruning\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            w[np.abs(w) < threshold] = 0\n",
    "            layer.set_weights([w, b])\n",
    "    return model\n",
    "\n",
    "def layerwise_prune(model, sparsity=0.7):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            thresh = np.percentile(np.abs(w), sparsity*100)\n",
    "            w[np.abs(w) < thresh] = 0\n",
    "            layer.set_weights([w, b])\n",
    "    return model\n",
    "\n",
    "# -----------------\n",
    "# Evaluation helpers\n",
    "# -----------------\n",
    "def count_zero_weights(model):\n",
    "    total = 0\n",
    "    zero = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            total += w.size + b.size\n",
    "            zero += np.sum(w == 0) + np.sum(b == 0)\n",
    "    return zero, total\n",
    "\n",
    "def measure_inference_time(model, dataset):\n",
    "    start = time.time()\n",
    "    _ = model.predict(dataset, verbose=0)\n",
    "    end = time.time()\n",
    "    return end - start\n",
    "\n",
    "# -----------------\n",
    "# Main workflow\n",
    "# -----------------\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "results = []\n",
    "\n",
    "# --- Train base CNN ---\n",
    "print(\"\\n🚀 Training base CNN...\")\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "base_model = build_cnn(input_shape, num_classes)\n",
    "base_model.fit(train_ds, validation_data=val_ds, epochs=5,\n",
    "               callbacks=[TrainingProgress()], verbose=0)\n",
    "\n",
    "# Save unpruned\n",
    "unpruned_path = \"saved_models/cnn_unpruned.h5\"\n",
    "base_model.save(unpruned_path)\n",
    "zero, total = count_zero_weights(base_model)\n",
    "time_inf = measure_inference_time(base_model, test_ds)\n",
    "test_loss, test_acc = base_model.evaluate(test_ds, verbose=0)\n",
    "results.append([\"Unpruned\", zero/total*100, test_acc, time_inf])\n",
    "print(f\"\\n📦 Unpruned: zeros={zero}/{total} ({zero/total*100:.2f}%), \"\n",
    "      f\"inference={time_inf:.4f}s, accuracy={test_acc:.4f}\")\n",
    "\n",
    "# --- Manual pruning 0.1 ---\n",
    "manual_model = tf.keras.models.clone_model(base_model)\n",
    "manual_model.set_weights(base_model.get_weights())\n",
    "manual_model = manual_prune(manual_model, threshold=0.1)\n",
    "manual_model.fit(train_ds, validation_data=val_ds, epochs=2,\n",
    "                 callbacks=[TrainingProgress()], verbose=0)\n",
    "manual_path = \"saved_models/cnn_manual_0.1.h5\"\n",
    "manual_model.save(manual_path)\n",
    "zero, total = count_zero_weights(manual_model)\n",
    "time_inf = measure_inference_time(manual_model, test_ds)\n",
    "test_loss, test_acc = manual_model.evaluate(test_ds, verbose=0)\n",
    "results.append([\"Manual 0.1\", zero/total*100, test_acc, time_inf])\n",
    "\n",
    "# --- Global 70% pruning ---\n",
    "global_model = tf.keras.models.clone_model(base_model)\n",
    "global_model.set_weights(base_model.get_weights())\n",
    "global_model = global_prune(global_model, sparsity=0.7)\n",
    "global_model.fit(train_ds, validation_data=val_ds, epochs=2,\n",
    "                 callbacks=[TrainingProgress()], verbose=0)\n",
    "global_path = \"saved_models/cnn_global_70.h5\"\n",
    "global_model.save(global_path)\n",
    "zero, total = count_zero_weights(global_model)\n",
    "time_inf = measure_inference_time(global_model, test_ds)\n",
    "test_loss, test_acc = global_model.evaluate(test_ds, verbose=0)\n",
    "results.append([\"Global 70%\", zero/total*100, test_acc, time_inf])\n",
    "\n",
    "# --- Layer-wise 70% pruning ---\n",
    "layer_model = tf.keras.models.clone_model(base_model)\n",
    "layer_model.set_weights(base_model.get_weights())\n",
    "layer_model = layerwise_prune(layer_model, sparsity=0.7)\n",
    "layer_model.fit(train_ds, validation_data=val_ds, epochs=2,\n",
    "                callbacks=[TrainingProgress()], verbose=0)\n",
    "layer_path = \"saved_models/cnn_layerwise_70.h5\"\n",
    "layer_model.save(layer_path)\n",
    "zero, total = count_zero_weights(layer_model)\n",
    "time_inf = measure_inference_time(layer_model, test_ds)\n",
    "test_loss, test_acc = layer_model.evaluate(test_ds, verbose=0)\n",
    "results.append([\"Layer-wise 70%\", zero/total*100, test_acc, time_inf])\n",
    "\n",
    "# --- Summary Table ---\n",
    "df = pd.DataFrame(results, columns=[\"Method\", \"% Zero Weights\", \"Test Accuracy\", \"Inference Time (s)\"])\n",
    "print(\"\\n✅ Pruning Comparison Summary:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9bf71ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training baseline CNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1: loss=1.2339, val_loss=0.8584, acc=0.5347, val_acc=0.6745\n",
      "✅ Epoch 2: loss=0.8798, val_loss=0.6708, acc=0.6739, val_acc=0.7450\n",
      "✅ Epoch 3: loss=0.7302, val_loss=0.5502, acc=0.7333, val_acc=0.8004\n",
      "✅ Epoch 4: loss=0.6078, val_loss=0.4849, acc=0.7826, val_acc=0.8179\n",
      "✅ Epoch 5: loss=0.5363, val_loss=0.4326, acc=0.8081, val_acc=0.8433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Unpruned: zeros=0/684361 (0.00%), inference=2.5799s, accuracy=0.8178\n",
      "\n",
      "✂️ Manual pruning at threshold=0.001 ...\n",
      "✅ Epoch 1: loss=0.4794, val_loss=0.3761, acc=0.8291, val_acc=0.8642\n",
      "✅ Epoch 2: loss=0.4313, val_loss=0.3971, acc=0.8470, val_acc=0.8580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Manual threshold 0.001: zeros=10861/684361 (1.59%), inference=2.5042s, accuracy=0.8120\n",
      "\n",
      "✂️ Manual pruning at threshold=0.01 ...\n",
      "✅ Epoch 1: loss=0.4836, val_loss=0.3991, acc=0.8293, val_acc=0.8530\n",
      "✅ Epoch 2: loss=0.4406, val_loss=0.3628, acc=0.8435, val_acc=0.8693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Manual threshold 0.01: zeros=109784/684361 (16.04%), inference=2.5241s, accuracy=0.8000\n",
      "\n",
      "✂️ Manual pruning at threshold=0.05 ...\n",
      "✅ Epoch 1: loss=0.4824, val_loss=0.3513, acc=0.8288, val_acc=0.8708\n",
      "✅ Epoch 2: loss=0.4302, val_loss=0.3731, acc=0.8481, val_acc=0.8677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Manual threshold 0.05: zeros=413534/684361 (60.43%), inference=2.5146s, accuracy=0.8220\n",
      "\n",
      "✂️ Manual pruning at threshold=0.1 ...\n",
      "✅ Epoch 1: loss=0.5559, val_loss=0.3796, acc=0.8050, val_acc=0.8652\n",
      "✅ Epoch 2: loss=0.4448, val_loss=0.3941, acc=0.8418, val_acc=0.8572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Manual threshold 0.1: zeros=449494/684361 (65.68%), inference=2.4905s, accuracy=0.8260\n",
      "\n",
      "🌐 Global pruning at 70% ...\n",
      "✅ Epoch 1: loss=0.4783, val_loss=0.3852, acc=0.8309, val_acc=0.8598\n",
      "✅ Epoch 2: loss=0.4359, val_loss=0.3757, acc=0.8438, val_acc=0.8628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Global 70%: zeros=396046/684361 (57.87%), inference=2.4351s, accuracy=0.8283\n",
      "\n",
      "🧱 Layer-wise pruning at 70% ...\n",
      "✅ Epoch 1: loss=0.5036, val_loss=0.4651, acc=0.8219, val_acc=0.8364\n",
      "✅ Epoch 2: loss=0.4411, val_loss=0.4410, acc=0.8430, val_acc=0.8431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Layer-wise 70%: zeros=398946/684361 (58.29%), inference=2.7555s, accuracy=0.8091\n",
      "\n",
      "✅ Pruning Comparison Summary:\n",
      "                Method  % Zero Weights  Test Accuracy  Inference Time (s)                                     Path\n",
      "              Unpruned        0.000000       0.817827            2.579865             saved_models/cnn_unpruned.h5\n",
      "Manual threshold 0.001        1.587028       0.811978            2.504195  saved_models/cnn_pruned_manual_0.001.h5\n",
      " Manual threshold 0.01       16.041826       0.800000            2.524127   saved_models/cnn_pruned_manual_0.01.h5\n",
      " Manual threshold 0.05       60.426295       0.822006            2.514647   saved_models/cnn_pruned_manual_0.05.h5\n",
      "  Manual threshold 0.1       65.680832       0.826045            2.490517    saved_models/cnn_pruned_manual_0.1.h5\n",
      "            Global 70%       57.870919       0.828273            2.435097    saved_models/cnn_pruned_global_0.7.h5\n",
      "        Layer-wise 70%       58.294672       0.809053            2.755503 saved_models/cnn_pruned_layerwise_0.7.h5\n",
      "\n",
      "📝 Summary saved to saved_models/pruning_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# ================\n",
    "# Config\n",
    "# ================\n",
    "FILE_PATH = \"/Users/arihangupta/.medmnist/pathmnist.npz\"\n",
    "SAVE_DIR = \"saved_models\"\n",
    "BASE_EPOCHS = 5\n",
    "PRUNE_FINETUNE_EPOCHS = 2\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "IMG_SIZE = 64\n",
    "MANUAL_THRESHOLDS = [0.001, 0.01, 0.05, 0.1]\n",
    "GLOBAL_SPARSITY = 0.7   # 70%\n",
    "LAYERWISE_SPARSITY = 0.7\n",
    "\n",
    "# Optional: reproducibility\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ================\n",
    "# Data\n",
    "# ================\n",
    "data = np.load(FILE_PATH)\n",
    "X_train, y_train = data[\"train_images\"], data[\"train_labels\"].flatten()\n",
    "X_val,   y_val   = data[\"val_images\"],   data[\"val_labels\"].flatten()\n",
    "X_test,  y_test  = data[\"test_images\"],  data[\"test_labels\"].flatten()\n",
    "\n",
    "# Normalize\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_val   = X_val.astype(\"float32\") / 255.0\n",
    "X_test  = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Ensure 3 channels\n",
    "def to_3ch(arr):\n",
    "    if arr.ndim == 3 or arr.shape[-1] == 1:\n",
    "        return np.repeat(arr[..., np.newaxis], 3, axis=-1)\n",
    "    return arr\n",
    "X_train, X_val, X_test = to_3ch(X_train), to_3ch(X_val), to_3ch(X_test)\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# tf.data pipelines with resize\n",
    "def make_dataset(images, labels, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(images))\n",
    "    ds = ds.map(lambda x, y: (tf.image.resize(x, [IMG_SIZE, IMG_SIZE]), y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_dataset(X_train, y_train, shuffle=True)\n",
    "val_ds   = make_dataset(X_val, y_val, shuffle=False)\n",
    "test_ds  = make_dataset(X_test, y_test, shuffle=False)\n",
    "\n",
    "# ================\n",
    "# Model\n",
    "# ================\n",
    "def build_cnn(input_shape, num_classes, learning_rate=LEARNING_RATE):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Conv2D(64, (3,3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Conv2D(128, (3,3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "class TrainingProgress(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"✅ Epoch {epoch+1}: \"\n",
    "              f\"loss={logs['loss']:.4f}, \"\n",
    "              f\"val_loss={logs['val_loss']:.4f}, \"\n",
    "              f\"acc={logs['accuracy']:.4f}, \"\n",
    "              f\"val_acc={logs['val_accuracy']:.4f}\")\n",
    "\n",
    "def clone_and_compile(model, learning_rate=LEARNING_RATE):\n",
    "    m = tf.keras.models.clone_model(model)\n",
    "    m.set_weights(model.get_weights())\n",
    "    m.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return m\n",
    "\n",
    "# ================\n",
    "# Pruning helpers\n",
    "# ================\n",
    "def manual_prune(model, threshold=0.1):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            w[np.abs(w) < threshold] = 0\n",
    "            layer.set_weights([w, b])\n",
    "    return model\n",
    "\n",
    "def global_prune(model, sparsity=0.7):\n",
    "    # Gather all conv/dense weights\n",
    "    all_w = []\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, _ = layer.get_weights()\n",
    "            all_w.append(np.abs(w).ravel())\n",
    "    if not all_w:\n",
    "        return model\n",
    "    all_w = np.concatenate(all_w)\n",
    "    thr = np.percentile(all_w, sparsity * 100.0)\n",
    "    # Apply\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            w[np.abs(w) < thr] = 0\n",
    "            layer.set_weights([w, b])\n",
    "    return model\n",
    "\n",
    "def layerwise_prune(model, sparsity=0.7):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            thr = np.percentile(np.abs(w), sparsity * 100.0)\n",
    "            w[np.abs(w) < thr] = 0\n",
    "            layer.set_weights([w, b])\n",
    "    return model\n",
    "\n",
    "def count_zero_weights(model):\n",
    "    total = 0\n",
    "    zero = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            total += w.size + b.size\n",
    "            zero  += np.sum(w == 0) + np.sum(b == 0)\n",
    "    return zero, total\n",
    "\n",
    "def measure_inference_time(model, dataset):\n",
    "    start = time.time()\n",
    "    _ = model.predict(dataset, verbose=0)\n",
    "    return time.time() - start\n",
    "\n",
    "def evaluate_model(model, name, path=None):\n",
    "    zero, total = count_zero_weights(model)\n",
    "    inf_time = measure_inference_time(model, test_ds)\n",
    "    _, acc = model.evaluate(test_ds, verbose=0)\n",
    "    if path:\n",
    "        model.save(path)\n",
    "    print(f\"📦 {name}: zeros={zero}/{total} ({(zero/total)*100:.2f}%), \"\n",
    "          f\"inference={inf_time:.4f}s, accuracy={acc:.4f}\")\n",
    "    return {\n",
    "        \"Method\": name,\n",
    "        \"% Zero Weights\": (zero/total)*100 if total > 0 else 0.0,\n",
    "        \"Test Accuracy\": acc,\n",
    "        \"Inference Time (s)\": inf_time,\n",
    "        \"Path\": path or \"\"\n",
    "    }\n",
    "\n",
    "# ================\n",
    "# Train baseline\n",
    "# ================\n",
    "print(\"\\n🚀 Training baseline CNN...\")\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "base_model = build_cnn(input_shape, num_classes, learning_rate=LEARNING_RATE)\n",
    "base_model.fit(train_ds, validation_data=val_ds, epochs=BASE_EPOCHS,\n",
    "               callbacks=[TrainingProgress()], verbose=0)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Baseline (unpruned)\n",
    "unpruned_path = os.path.join(SAVE_DIR, \"cnn_unpruned.h5\")\n",
    "results.append(evaluate_model(base_model, \"Unpruned\", path=unpruned_path))\n",
    "\n",
    "# ================\n",
    "# Manual pruning (fixed thresholds)\n",
    "# ================\n",
    "for t in MANUAL_THRESHOLDS:\n",
    "    print(f\"\\n✂️ Manual pruning at threshold={t} ...\")\n",
    "    m = clone_and_compile(base_model, learning_rate=LEARNING_RATE)\n",
    "    m = manual_prune(m, threshold=t)\n",
    "    if PRUNE_FINETUNE_EPOCHS > 0:\n",
    "        m.fit(train_ds, validation_data=val_ds, epochs=PRUNE_FINETUNE_EPOCHS,\n",
    "              callbacks=[TrainingProgress()], verbose=0)\n",
    "    path = os.path.join(SAVE_DIR, f\"cnn_pruned_manual_{t}.h5\")\n",
    "    results.append(evaluate_model(m, f\"Manual threshold {t}\", path=path))\n",
    "\n",
    "# ================\n",
    "# Global 70% pruning\n",
    "# ================\n",
    "print(\"\\n🌐 Global pruning at 70% ...\")\n",
    "g = clone_and_compile(base_model, learning_rate=LEARNING_RATE)\n",
    "g = global_prune(g, sparsity=GLOBAL_SPARSITY)\n",
    "if PRUNE_FINETUNE_EPOCHS > 0:\n",
    "    g.fit(train_ds, validation_data=val_ds, epochs=PRUNE_FINETUNE_EPOCHS,\n",
    "          callbacks=[TrainingProgress()], verbose=0)\n",
    "g_path = os.path.join(SAVE_DIR, \"cnn_pruned_global_0.7.h5\")\n",
    "results.append(evaluate_model(g, \"Global 70%\", path=g_path))\n",
    "\n",
    "# ================\n",
    "# Layer-wise 70% pruning\n",
    "# ================\n",
    "print(\"\\n🧱 Layer-wise pruning at 70% ...\")\n",
    "l = clone_and_compile(base_model, learning_rate=LEARNING_RATE)\n",
    "l = layerwise_prune(l, sparsity=LAYERWISE_SPARSITY)\n",
    "if PRUNE_FINETUNE_EPOCHS > 0:\n",
    "    l.fit(train_ds, validation_data=val_ds, epochs=PRUNE_FINETUNE_EPOCHS,\n",
    "          callbacks=[TrainingProgress()], verbose=0)\n",
    "l_path = os.path.join(SAVE_DIR, \"cnn_pruned_layerwise_0.7.h5\")\n",
    "results.append(evaluate_model(l, \"Layer-wise 70%\", path=l_path))\n",
    "\n",
    "# ================\n",
    "# Summary\n",
    "# ================\n",
    "df = pd.DataFrame(results, columns=[\"Method\", \"% Zero Weights\", \"Test Accuracy\", \"Inference Time (s)\", \"Path\"])\n",
    "print(\"\\n✅ Pruning Comparison Summary:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Save summary to CSV for reference\n",
    "summary_csv = os.path.join(SAVE_DIR, \"pruning_summary.csv\")\n",
    "df.to_csv(summary_csv, index=False)\n",
    "print(f\"\\n📝 Summary saved to {summary_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c94b82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training baseline CNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1: loss=1.2339, val_loss=0.8584, acc=0.5347, val_acc=0.6745\n",
      "✅ Epoch 2: loss=0.8798, val_loss=0.6708, acc=0.6739, val_acc=0.7450\n",
      "✅ Epoch 3: loss=0.7302, val_loss=0.5502, acc=0.7333, val_acc=0.8004\n",
      "✅ Epoch 4: loss=0.6078, val_loss=0.4849, acc=0.7826, val_acc=0.8179\n",
      "✅ Epoch 5: loss=0.5363, val_loss=0.4326, acc=0.8081, val_acc=0.8433\n",
      "\n",
      "Model has 10 total layers.\n",
      "5 layers have trainable weights. They are:\n",
      "  - index=0, name='conv2d_1', weight-shapes=[(3, 3, 3, 32), (32,)]\n",
      "  - index=2, name='conv2d_2', weight-shapes=[(3, 3, 32, 64), (64,)]\n",
      "  - index=4, name='conv2d_3', weight-shapes=[(3, 3, 64, 128), (128,)]\n",
      "  - index=7, name='dense_1', weight-shapes=[(4608, 128), (128,)]\n",
      "  - index=9, name='predictions', weight-shapes=[(128, 9), (9,)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Unpruned (baseline) | zeros=0/684361 (0.00%), inference=5.7261s, accuracy=0.8178, saved-> saved_models_layer_prune/model_unpruned.h5\n",
      "\n",
      "✂️ Pruning entire layer index=0 name='conv2d_1' (zeroing its weights)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for layer-pruned model: zeros=896/684361 (0.13%), inference=5.4527s, accuracy=0.1180\n",
      "\n",
      "✂️ Pruning entire layer index=2 name='conv2d_2' (zeroing its weights)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for layer-pruned model: zeros=18496/684361 (2.70%), inference=5.5501s, accuracy=0.1180\n",
      "\n",
      "✂️ Pruning entire layer index=4 name='conv2d_3' (zeroing its weights)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for layer-pruned model: zeros=73856/684361 (10.79%), inference=5.3598s, accuracy=0.1717\n",
      "\n",
      "✂️ Pruning entire layer index=7 name='dense_1' (zeroing its weights)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for layer-pruned model: zeros=589952/684361 (86.20%), inference=5.6108s, accuracy=0.1717\n",
      "\n",
      "✂️ Pruning entire layer index=9 name='predictions' (zeroing its weights)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for layer-pruned model: zeros=1161/684361 (0.17%), inference=5.2825s, accuracy=0.1864\n",
      "\n",
      "✅ Layer-pruning comparison summary:\n",
      "              Model  LayerIndex   LayerName  HasWeights  % Zero Weights  Test Accuracy  Inference Time (s)                                                  Path\n",
      "Unpruned (baseline)          -1         ALL        True        0.000000       0.817827            5.726130            saved_models_layer_prune/model_unpruned.h5\n",
      "       Layer-pruned           0    conv2d_1        True        0.130925       0.117967            5.452749    saved_models_layer_prune/prune_layer_0_conv2d_1.h5\n",
      "       Layer-pruned           2    conv2d_2        True        2.702667       0.117967            5.550104    saved_models_layer_prune/prune_layer_2_conv2d_2.h5\n",
      "       Layer-pruned           4    conv2d_3        True       10.791965       0.171727            5.359769    saved_models_layer_prune/prune_layer_4_conv2d_3.h5\n",
      "       Layer-pruned           7     dense_1        True       86.204795       0.171727            5.610800     saved_models_layer_prune/prune_layer_7_dense_1.h5\n",
      "       Layer-pruned           9 predictions        True        0.169647       0.186351            5.282547 saved_models_layer_prune/prune_layer_9_predictions.h5\n",
      "\n",
      "Saved summary CSV to: saved_models_layer_prune/layer_prune_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# layer_prune_experiment.py\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "FILE_PATH = \"/Users/arihangupta/.medmnist/pathmnist.npz\"\n",
    "SAVE_DIR = \"saved_models_layer_prune\"\n",
    "BASE_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "IMG_SIZE = 64\n",
    "# If >0, each pruned model will be fine-tuned for this many epochs after zeroing the layer.\n",
    "FINE_TUNE_AFTER_PRUNE = 0\n",
    "# ----------------------------------------\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------- Data ----------------\n",
    "data = np.load(FILE_PATH)\n",
    "X_train, y_train = data[\"train_images\"], data[\"train_labels\"].flatten()\n",
    "X_val, y_val     = data[\"val_images\"], data[\"val_labels\"].flatten()\n",
    "X_test, y_test   = data[\"test_images\"], data[\"test_labels\"].flatten()\n",
    "\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_val   = X_val.astype(\"float32\") / 255.0\n",
    "X_test  = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "def to_3ch(arr):\n",
    "    if arr.ndim == 3 or arr.shape[-1] == 1:\n",
    "        return np.repeat(arr[..., np.newaxis], 3, axis=-1)\n",
    "    return arr\n",
    "\n",
    "X_train, X_val, X_test = to_3ch(X_train), to_3ch(X_val), to_3ch(X_test)\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "def make_dataset(images, labels, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(images))\n",
    "    ds = ds.map(lambda x, y: (tf.image.resize(x, [IMG_SIZE, IMG_SIZE]), y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_dataset(X_train, y_train, shuffle=True)\n",
    "val_ds   = make_dataset(X_val, y_val, shuffle=False)\n",
    "test_ds  = make_dataset(X_test, y_test, shuffle=False)\n",
    "\n",
    "# ---------------- Model definition ----------------\n",
    "def build_cnn(input_shape, num_classes, learning_rate=LEARNING_RATE):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=input_shape, name=\"conv2d_1\"),\n",
    "        layers.MaxPooling2D((2,2), name=\"pool_1\"),\n",
    "        layers.Conv2D(64, (3,3), activation=\"relu\", name=\"conv2d_2\"),\n",
    "        layers.MaxPooling2D((2,2), name=\"pool_2\"),\n",
    "        layers.Conv2D(128, (3,3), activation=\"relu\", name=\"conv2d_3\"),\n",
    "        layers.MaxPooling2D((2,2), name=\"pool_3\"),\n",
    "        layers.Flatten(name=\"flatten\"),\n",
    "        layers.Dense(128, activation=\"relu\", name=\"dense_1\"),\n",
    "        layers.Dropout(0.5, name=\"dropout\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\", name=\"predictions\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "class TrainingProgress(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"✅ Epoch {epoch+1}: loss={logs['loss']:.4f}, val_loss={logs['val_loss']:.4f}, \"\n",
    "              f\"acc={logs['accuracy']:.4f}, val_acc={logs['val_accuracy']:.4f}\")\n",
    "\n",
    "# ---------------- Utilities ----------------\n",
    "def get_weightful_layers(model):\n",
    "    \"\"\"Return list of tuples (index, layer) for layers that have weights.\"\"\"\n",
    "    weightful = []\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if layer.weights:  # non-empty list of tf.Variables\n",
    "            weightful.append((i, layer))\n",
    "    return weightful\n",
    "\n",
    "def zero_layer_weights(model, layer_index):\n",
    "    \"\"\"Set all weights and biases of layer at layer_index to zero in-place.\"\"\"\n",
    "    layer = model.layers[layer_index]\n",
    "    if not layer.weights:\n",
    "        return False  # nothing to zero\n",
    "    new_weights = []\n",
    "    for w in layer.get_weights():\n",
    "        new_weights.append(np.zeros_like(w))\n",
    "    layer.set_weights(new_weights)\n",
    "    return True\n",
    "\n",
    "def count_zero_weights(model):\n",
    "    total = 0\n",
    "    zero = 0\n",
    "    for layer in model.layers:\n",
    "        if layer.weights:\n",
    "            wts = layer.get_weights()\n",
    "            for arr in wts:\n",
    "                total += arr.size\n",
    "                zero += np.sum(arr == 0)\n",
    "    return int(zero), int(total)\n",
    "\n",
    "def measure_inference_time(model, dataset):\n",
    "    start = time.time()\n",
    "    _ = model.predict(dataset, verbose=0)\n",
    "    return time.time() - start\n",
    "\n",
    "def clone_and_compile(model):\n",
    "    m = tf.keras.models.clone_model(model)\n",
    "    m.set_weights(model.get_weights())\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "              loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return m\n",
    "\n",
    "def evaluate_and_save(model, name, path):\n",
    "    zero, total = count_zero_weights(model)\n",
    "    inf_time = measure_inference_time(model, test_ds)\n",
    "    loss, acc = model.evaluate(test_ds, verbose=0)\n",
    "    print(f\"📦 {name} | zeros={zero}/{total} ({(zero/total)*100:.2f}%), \"\n",
    "          f\"inference={inf_time:.4f}s, accuracy={acc:.4f}, saved-> {path}\")\n",
    "    model.save(path)\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"LayerIndex\": None,\n",
    "        \"LayerName\": None,\n",
    "        \"HasWeights\": None,\n",
    "        \"% Zero Weights\": (zero/total)*100 if total else 0.0,\n",
    "        \"Test Accuracy\": float(acc),\n",
    "        \"Inference Time (s)\": float(inf_time),\n",
    "        \"Path\": path\n",
    "    }\n",
    "\n",
    "# ---------------- Train baseline ----------------\n",
    "print(\"\\n🚀 Training baseline CNN...\")\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "baseline = build_cnn(input_shape, num_classes)\n",
    "baseline.fit(train_ds, validation_data=val_ds, epochs=BASE_EPOCHS,\n",
    "             callbacks=[TrainingProgress()], verbose=0)\n",
    "\n",
    "# Baseline info\n",
    "weightful = get_weightful_layers(baseline)\n",
    "print(f\"\\nModel has {len(baseline.layers)} total layers.\")\n",
    "print(f\"{len(weightful)} layers have trainable weights. They are:\")\n",
    "for idx, lyr in weightful:\n",
    "    print(f\"  - index={idx}, name='{lyr.name}', weight-shapes={[w.shape for w in lyr.get_weights()]}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# Save baseline\n",
    "baseline_path = os.path.join(SAVE_DIR, \"model_unpruned.h5\")\n",
    "res = evaluate_and_save(baseline, \"Unpruned (baseline)\", baseline_path)\n",
    "# populate layer-level fields for baseline\n",
    "res[\"LayerIndex\"] = -1\n",
    "res[\"LayerName\"] = \"ALL\"\n",
    "res[\"HasWeights\"] = True\n",
    "results.append(res)\n",
    "\n",
    "# ---------------- For each weightful layer: prune that whole layer ----------------\n",
    "for layer_index, layer in weightful:\n",
    "    name = f\"prune_layer_{layer_index}_{layer.name}\"\n",
    "    print(f\"\\n✂️ Pruning entire layer index={layer_index} name='{layer.name}' (zeroing its weights)...\")\n",
    "    # clone baseline\n",
    "    m = clone_and_compile(baseline)\n",
    "    had_weights = bool(m.layers[layer_index].weights)\n",
    "    # zero the layer weights\n",
    "    success = zero_layer_weights(m, layer_index)\n",
    "    if not success:\n",
    "        print(f\"  (Layer {layer_index} had no weights; skipping.)\")\n",
    "    # Optional fine-tune\n",
    "    if FINE_TUNE_AFTER_PRUNE > 0:\n",
    "        print(f\"  🛠 Fine-tuning pruned model for {FINE_TUNE_AFTER_PRUNE} epochs...\")\n",
    "        m.fit(train_ds, validation_data=val_ds, epochs=FINE_TUNE_AFTER_PRUNE,\n",
    "              callbacks=[TrainingProgress()], verbose=0)\n",
    "    # save & evaluate\n",
    "    path = os.path.join(SAVE_DIR, f\"{name}.h5\")\n",
    "    zero, total = count_zero_weights(m)\n",
    "    inf_time = measure_inference_time(m, test_ds)\n",
    "    loss, acc = m.evaluate(test_ds, verbose=0)\n",
    "    print(f\"Result for layer-pruned model: zeros={zero}/{total} ({(zero/total)*100:.2f}%), inference={inf_time:.4f}s, accuracy={acc:.4f}\")\n",
    "    m.save(path)\n",
    "    results.append({\n",
    "        \"Model\": \"Layer-pruned\",\n",
    "        \"LayerIndex\": int(layer_index),\n",
    "        \"LayerName\": layer.name,\n",
    "        \"HasWeights\": bool(had_weights),\n",
    "        \"% Zero Weights\": (zero/total)*100 if total else 0.0,\n",
    "        \"Test Accuracy\": float(acc),\n",
    "        \"Inference Time (s)\": float(inf_time),\n",
    "        \"Path\": path\n",
    "    })\n",
    "\n",
    "# ---------------- Summary table ----------------\n",
    "df = pd.DataFrame(results, columns=[\n",
    "    \"Model\", \"LayerIndex\", \"LayerName\", \"HasWeights\",\n",
    "    \"% Zero Weights\", \"Test Accuracy\", \"Inference Time (s)\", \"Path\"\n",
    "])\n",
    "print(\"\\n✅ Layer-pruning comparison summary:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "csv_path = os.path.join(SAVE_DIR, \"layer_prune_summary.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"\\nSaved summary CSV to: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c96ac011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training baseline CNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1: loss=1.2339, val_loss=0.8584, acc=0.5347, val_acc=0.6745\n",
      "✅ Epoch 2: loss=0.8798, val_loss=0.6708, acc=0.6739, val_acc=0.7450\n",
      "✅ Epoch 3: loss=0.7302, val_loss=0.5502, acc=0.7333, val_acc=0.8004\n",
      "✅ Epoch 4: loss=0.6078, val_loss=0.4849, acc=0.7826, val_acc=0.8179\n",
      "✅ Epoch 5: loss=0.5363, val_loss=0.4326, acc=0.8081, val_acc=0.8433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Unpruned: zeros=0/684361 (0.00%), inference=2.6422s, accuracy=0.8178\n",
      "\n",
      "✂️ Manual pruning at threshold=0.001 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Manual threshold 0.001: zeros=13290/684361 (1.94%), inference=2.4103s, accuracy=0.8177\n",
      "\n",
      "✂️ Manual pruning at threshold=0.01 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Manual threshold 0.01: zeros=133076/684361 (19.45%), inference=2.3821s, accuracy=0.8134\n",
      "\n",
      "✂️ Manual pruning at threshold=0.05 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Manual threshold 0.05: zeros=508065/684361 (74.24%), inference=2.5810s, accuracy=0.3403\n",
      "\n",
      "✂️ Manual pruning at threshold=0.1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Manual threshold 0.1: zeros=597356/684361 (87.29%), inference=2.4078s, accuracy=0.1415\n",
      "\n",
      "🌐 Global pruning at 70% ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Global 70%: zeros=478800/684361 (69.96%), inference=2.5726s, accuracy=0.7646\n",
      "\n",
      "🧱 Layer-wise pruning at 70% ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Layer-wise 70%: zeros=478799/684361 (69.96%), inference=2.4244s, accuracy=0.1421\n",
      "\n",
      "✅ Pruning Comparison Summary:\n",
      "                Method  % Zero Weights  Test Accuracy  Inference Time (s)                                     Path\n",
      "              Unpruned        0.000000       0.817827            2.642210             saved_models/cnn_unpruned.h5\n",
      "Manual threshold 0.001        1.941958       0.817688            2.410261  saved_models/cnn_pruned_manual_0.001.h5\n",
      " Manual threshold 0.01       19.445293       0.813370            2.382140   saved_models/cnn_pruned_manual_0.01.h5\n",
      " Manual threshold 0.05       74.239327       0.340251            2.581028   saved_models/cnn_pruned_manual_0.05.h5\n",
      "  Manual threshold 0.1       87.286681       0.141504            2.407829    saved_models/cnn_pruned_manual_0.1.h5\n",
      "            Global 70%       69.963075       0.764624            2.572570    saved_models/cnn_pruned_global_0.7.h5\n",
      "        Layer-wise 70%       69.962929       0.142061            2.424415 saved_models/cnn_pruned_layerwise_0.7.h5\n",
      "\n",
      "📝 Summary saved to saved_models/pruning_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# ================\n",
    "# Config\n",
    "# ================\n",
    "FILE_PATH = \"/Users/arihangupta/.medmnist/pathmnist.npz\"\n",
    "SAVE_DIR = \"saved_models\"\n",
    "BASE_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "IMG_SIZE = 64\n",
    "MANUAL_THRESHOLDS = [0.001, 0.01, 0.05, 0.1]\n",
    "GLOBAL_SPARSITY = 0.7   # 70%\n",
    "LAYERWISE_SPARSITY = 0.7\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ================\n",
    "# Data\n",
    "# ================\n",
    "data = np.load(FILE_PATH)\n",
    "X_train, y_train = data[\"train_images\"], data[\"train_labels\"].flatten()\n",
    "X_val,   y_val   = data[\"val_images\"],   data[\"val_labels\"].flatten()\n",
    "X_test,  y_test  = data[\"test_images\"],  data[\"test_labels\"].flatten()\n",
    "\n",
    "# Normalize\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_val   = X_val.astype(\"float32\") / 255.0\n",
    "X_test  = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Ensure 3 channels\n",
    "def to_3ch(arr):\n",
    "    if arr.ndim == 3 or arr.shape[-1] == 1:\n",
    "        return np.repeat(arr[..., np.newaxis], 3, axis=-1)\n",
    "    return arr\n",
    "X_train, X_val, X_test = to_3ch(X_train), to_3ch(X_val), to_3ch(X_test)\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# tf.data pipelines with resize\n",
    "def make_dataset(images, labels, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(images))\n",
    "    ds = ds.map(lambda x, y: (tf.image.resize(x, [IMG_SIZE, IMG_SIZE]), y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_dataset(X_train, y_train, shuffle=True)\n",
    "val_ds   = make_dataset(X_val, y_val, shuffle=False)\n",
    "test_ds  = make_dataset(X_test, y_test, shuffle=False)\n",
    "\n",
    "# ================\n",
    "# Model\n",
    "# ================\n",
    "def build_cnn(input_shape, num_classes, learning_rate=LEARNING_RATE):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Conv2D(64, (3,3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Conv2D(128, (3,3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "class TrainingProgress(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"✅ Epoch {epoch+1}: \"\n",
    "              f\"loss={logs['loss']:.4f}, \"\n",
    "              f\"val_loss={logs['val_loss']:.4f}, \"\n",
    "              f\"acc={logs['accuracy']:.4f}, \"\n",
    "              f\"val_acc={logs['val_accuracy']:.4f}\")\n",
    "\n",
    "def clone_and_compile(model, learning_rate=LEARNING_RATE):\n",
    "    m = tf.keras.models.clone_model(model)\n",
    "    m.set_weights(model.get_weights())\n",
    "    m.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return m\n",
    "\n",
    "# ================\n",
    "# Pruning helpers\n",
    "# ================\n",
    "def manual_prune(model, threshold=0.1):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            w[np.abs(w) < threshold] = 0\n",
    "            layer.set_weights([w, b])\n",
    "    return model\n",
    "\n",
    "def global_prune(model, sparsity=0.7):\n",
    "    all_w = []\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, _ = layer.get_weights()\n",
    "            all_w.append(np.abs(w).ravel())\n",
    "    if not all_w:\n",
    "        return model\n",
    "    all_w = np.concatenate(all_w)\n",
    "    thr = np.percentile(all_w, sparsity * 100.0)\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            w[np.abs(w) < thr] = 0\n",
    "            layer.set_weights([w, b])\n",
    "    return model\n",
    "\n",
    "def layerwise_prune(model, sparsity=0.7):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            thr = np.percentile(np.abs(w), sparsity * 100.0)\n",
    "            w[np.abs(w) < thr] = 0\n",
    "            layer.set_weights([w, b])\n",
    "    return model\n",
    "\n",
    "def count_zero_weights(model):\n",
    "    total = 0\n",
    "    zero = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            total += w.size + b.size\n",
    "            zero  += np.sum(w == 0) + np.sum(b == 0)\n",
    "    return zero, total\n",
    "\n",
    "def measure_inference_time(model, dataset):\n",
    "    start = time.time()\n",
    "    _ = model.predict(dataset, verbose=0)\n",
    "    return time.time() - start\n",
    "\n",
    "def evaluate_model(model, name, path=None):\n",
    "    zero, total = count_zero_weights(model)\n",
    "    inf_time = measure_inference_time(model, test_ds)\n",
    "    _, acc = model.evaluate(test_ds, verbose=0)\n",
    "    if path:\n",
    "        model.save(path)\n",
    "    print(f\"📦 {name}: zeros={zero}/{total} ({(zero/total)*100:.2f}%), \"\n",
    "          f\"inference={inf_time:.4f}s, accuracy={acc:.4f}\")\n",
    "    return {\n",
    "        \"Method\": name,\n",
    "        \"% Zero Weights\": (zero/total)*100 if total > 0 else 0.0,\n",
    "        \"Test Accuracy\": acc,\n",
    "        \"Inference Time (s)\": inf_time,\n",
    "        \"Path\": path or \"\"\n",
    "    }\n",
    "\n",
    "# ================\n",
    "# Train baseline\n",
    "# ================\n",
    "print(\"\\n🚀 Training baseline CNN...\")\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "base_model = build_cnn(input_shape, num_classes, learning_rate=LEARNING_RATE)\n",
    "base_model.fit(train_ds, validation_data=val_ds, epochs=BASE_EPOCHS,\n",
    "               callbacks=[TrainingProgress()], verbose=0)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Baseline (unpruned)\n",
    "unpruned_path = os.path.join(SAVE_DIR, \"cnn_unpruned.h5\")\n",
    "results.append(evaluate_model(base_model, \"Unpruned\", path=unpruned_path))\n",
    "\n",
    "# ================\n",
    "# Manual pruning (fixed thresholds)\n",
    "# ================\n",
    "for t in MANUAL_THRESHOLDS:\n",
    "    print(f\"\\n✂️ Manual pruning at threshold={t} ...\")\n",
    "    m = clone_and_compile(base_model, learning_rate=LEARNING_RATE)\n",
    "    m = manual_prune(m, threshold=t)\n",
    "    path = os.path.join(SAVE_DIR, f\"cnn_pruned_manual_{t}.h5\")\n",
    "    results.append(evaluate_model(m, f\"Manual threshold {t}\", path=path))\n",
    "\n",
    "# ================\n",
    "# Global 70% pruning\n",
    "# ================\n",
    "print(\"\\n🌐 Global pruning at 70% ...\")\n",
    "g = clone_and_compile(base_model, learning_rate=LEARNING_RATE)\n",
    "g = global_prune(g, sparsity=GLOBAL_SPARSITY)\n",
    "g_path = os.path.join(SAVE_DIR, \"cnn_pruned_global_0.7.h5\")\n",
    "results.append(evaluate_model(g, \"Global 70%\", path=g_path))\n",
    "\n",
    "# ================\n",
    "# Layer-wise 70% pruning\n",
    "# ================\n",
    "print(\"\\n🧱 Layer-wise pruning at 70% ...\")\n",
    "l = clone_and_compile(base_model, learning_rate=LEARNING_RATE)\n",
    "l = layerwise_prune(l, sparsity=LAYERWISE_SPARSITY)\n",
    "l_path = os.path.join(SAVE_DIR, \"cnn_pruned_layerwise_0.7.h5\")\n",
    "results.append(evaluate_model(l, \"Layer-wise 70%\", path=l_path))\n",
    "\n",
    "# ================\n",
    "# Summary\n",
    "# ================\n",
    "df = pd.DataFrame(results, columns=[\"Method\", \"% Zero Weights\", \"Test Accuracy\", \"Inference Time (s)\", \"Path\"])\n",
    "print(\"\\n✅ Pruning Comparison Summary:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "summary_csv = os.path.join(SAVE_DIR, \"pruning_summary.csv\")\n",
    "df.to_csv(summary_csv, index=False)\n",
    "print(f\"\\n📝 Summary saved to {summary_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e8920e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 7007\n",
      "Val: 1003\n",
      "Test: 2005\n",
      "Sample image shape: (28, 28, 3) Label: [0]\n"
     ]
    }
   ],
   "source": [
    "import medmnist\n",
    "from medmnist import DermaMNIST\n",
    "import numpy as np\n",
    "\n",
    "# Choose the image size (28 for classic, or 64/128/224 for MedMNIST+)\n",
    "IMG_SIZE = 28  \n",
    "\n",
    "# Download DermMNIST (train/val/test) automatically\n",
    "train_dataset = DermaMNIST(split=\"train\", download=True, size=IMG_SIZE)\n",
    "val_dataset   = DermaMNIST(split=\"val\", download=True, size=IMG_SIZE)\n",
    "test_dataset  = DermaMNIST(split=\"test\", download=True, size=IMG_SIZE)\n",
    "\n",
    "print(\"Train:\", len(train_dataset))\n",
    "print(\"Val:\", len(val_dataset))\n",
    "print(\"Test:\", len(test_dataset))\n",
    "\n",
    "# Each dataset item is a tuple (PIL.Image, label)\n",
    "img, label = train_dataset[0]\n",
    "\n",
    "# Convert PIL image → NumPy array\n",
    "img_array = np.array(img)\n",
    "\n",
    "print(\"Sample image shape:\", img_array.shape, \"Label:\", label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3ff06b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training baseline CNN...\n",
      "✅ Epoch 1: loss=1.0434, val_loss=0.9344, acc=0.6699, val_acc=0.6690\n",
      "✅ Epoch 2: loss=0.9522, val_loss=0.8726, acc=0.6679, val_acc=0.6680\n",
      "✅ Epoch 3: loss=0.9224, val_loss=0.9168, acc=0.6746, val_acc=0.6690\n",
      "✅ Epoch 4: loss=0.8825, val_loss=0.8445, acc=0.6846, val_acc=0.6919\n",
      "✅ Epoch 5: loss=0.8561, val_loss=0.8558, acc=0.6917, val_acc=0.6889\n",
      "✅ Epoch 6: loss=0.8385, val_loss=0.8166, acc=0.6977, val_acc=0.6959\n",
      "✅ Epoch 7: loss=0.8203, val_loss=0.8196, acc=0.7032, val_acc=0.6979\n",
      "✅ Epoch 8: loss=0.8086, val_loss=0.7815, acc=0.7087, val_acc=0.7069\n",
      "✅ Epoch 9: loss=0.7964, val_loss=0.7699, acc=0.7053, val_acc=0.7079\n",
      "✅ Epoch 10: loss=0.7732, val_loss=0.7742, acc=0.7137, val_acc=0.7069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Unpruned: zeros=1/684103 (0.00%), inference=1.4692s, accuracy=0.7117\n",
      "\n",
      "✂️ Manual pruning at threshold=0.001 ...\n",
      "✅ Epoch 1: loss=0.7639, val_loss=0.7743, acc=0.7193, val_acc=0.7089\n",
      "✅ Epoch 2: loss=0.7492, val_loss=0.7506, acc=0.7254, val_acc=0.7129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Manual threshold 0.001: zeros=11708/684103 (1.71%), inference=1.4461s, accuracy=0.7207\n",
      "\n",
      "✂️ Manual pruning at threshold=0.01 ...\n",
      "✅ Epoch 1: loss=0.7616, val_loss=0.7520, acc=0.7174, val_acc=0.7079\n",
      "✅ Epoch 2: loss=0.7506, val_loss=0.7599, acc=0.7160, val_acc=0.6999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Manual threshold 0.01: zeros=117007/684103 (17.10%), inference=1.3803s, accuracy=0.7042\n",
      "\n",
      "✂️ Manual pruning at threshold=0.05 ...\n",
      "✅ Epoch 1: loss=0.7779, val_loss=0.7541, acc=0.7117, val_acc=0.7258\n",
      "✅ Epoch 2: loss=0.7511, val_loss=0.7662, acc=0.7206, val_acc=0.7149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Manual threshold 0.05: zeros=450877/684103 (65.91%), inference=1.3795s, accuracy=0.7167\n",
      "\n",
      "✂️ Manual pruning at threshold=0.1 ...\n",
      "✅ Epoch 1: loss=0.9025, val_loss=0.7820, acc=0.6846, val_acc=0.7069\n",
      "✅ Epoch 2: loss=0.8102, val_loss=0.7691, acc=0.7070, val_acc=0.7029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Manual threshold 0.1: zeros=490744/684103 (71.74%), inference=1.3625s, accuracy=0.7112\n",
      "\n",
      "🌐 Global pruning at 70% ...\n",
      "✅ Epoch 1: loss=0.7698, val_loss=0.7645, acc=0.7157, val_acc=0.7159\n",
      "✅ Epoch 2: loss=0.7534, val_loss=0.7516, acc=0.7191, val_acc=0.7119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Global 70%: zeros=361484/684103 (52.84%), inference=1.3651s, accuracy=0.7182\n",
      "\n",
      "🧱 Layer-wise pruning at 70% ...\n",
      "✅ Epoch 1: loss=0.8068, val_loss=0.7652, acc=0.7049, val_acc=0.7049\n",
      "✅ Epoch 2: loss=0.7700, val_loss=0.7590, acc=0.7151, val_acc=0.7069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Layer-wise 70%: zeros=360243/684103 (52.66%), inference=1.3844s, accuracy=0.7102\n",
      "\n",
      "✅ Pruning Comparison Summary:\n",
      "                Method  % Zero Weights  Test Accuracy  Inference Time (s)                                     Path\n",
      "              Unpruned        0.000146       0.711721            1.469197             saved_models/cnn_unpruned.h5\n",
      "Manual threshold 0.001        1.711438       0.720698            1.446091  saved_models/cnn_pruned_manual_0.001.h5\n",
      " Manual threshold 0.01       17.103711       0.704239            1.380266   saved_models/cnn_pruned_manual_0.01.h5\n",
      " Manual threshold 0.05       65.907765       0.716708            1.379477   saved_models/cnn_pruned_manual_0.05.h5\n",
      "  Manual threshold 0.1       71.735397       0.711222            1.362465    saved_models/cnn_pruned_manual_0.1.h5\n",
      "            Global 70%       52.840581       0.718204            1.365103    saved_models/cnn_pruned_global_0.7.h5\n",
      "        Layer-wise 70%       52.659176       0.710224            1.384379 saved_models/cnn_pruned_layerwise_0.7.h5\n",
      "\n",
      "📝 Summary saved to saved_models/pruning_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# ================\n",
    "# Config\n",
    "# ================\n",
    "FILE_PATH = \"/Users/arihangupta/.medmnist/dermamnist.npz\"\n",
    "SAVE_DIR = \"saved_models\"\n",
    "BASE_EPOCHS = 10\n",
    "PRUNE_FINETUNE_EPOCHS = 2\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "IMG_SIZE = 64\n",
    "MANUAL_THRESHOLDS = [0.001, 0.01, 0.05, 0.1]\n",
    "GLOBAL_SPARSITY = 0.7   # 70%\n",
    "LAYERWISE_SPARSITY = 0.7\n",
    "\n",
    "# Optional: reproducibility\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ================\n",
    "# Data\n",
    "# ================\n",
    "data = np.load(FILE_PATH)\n",
    "X_train, y_train = data[\"train_images\"], data[\"train_labels\"].flatten()\n",
    "X_val,   y_val   = data[\"val_images\"],   data[\"val_labels\"].flatten()\n",
    "X_test,  y_test  = data[\"test_images\"],  data[\"test_labels\"].flatten()\n",
    "\n",
    "# Normalize\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_val   = X_val.astype(\"float32\") / 255.0\n",
    "X_test  = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Ensure 3 channels\n",
    "def to_3ch(arr):\n",
    "    if arr.ndim == 3 or arr.shape[-1] == 1:\n",
    "        return np.repeat(arr[..., np.newaxis], 3, axis=-1)\n",
    "    return arr\n",
    "X_train, X_val, X_test = to_3ch(X_train), to_3ch(X_val), to_3ch(X_test)\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# tf.data pipelines with resize\n",
    "def make_dataset(images, labels, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(images))\n",
    "    ds = ds.map(lambda x, y: (tf.image.resize(x, [IMG_SIZE, IMG_SIZE]), y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_dataset(X_train, y_train, shuffle=True)\n",
    "val_ds   = make_dataset(X_val, y_val, shuffle=False)\n",
    "test_ds  = make_dataset(X_test, y_test, shuffle=False)\n",
    "\n",
    "# ================\n",
    "# Model\n",
    "# ================\n",
    "def build_cnn(input_shape, num_classes, learning_rate=LEARNING_RATE):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Conv2D(64, (3,3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Conv2D(128, (3,3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "class TrainingProgress(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"✅ Epoch {epoch+1}: \"\n",
    "              f\"loss={logs['loss']:.4f}, \"\n",
    "              f\"val_loss={logs['val_loss']:.4f}, \"\n",
    "              f\"acc={logs['accuracy']:.4f}, \"\n",
    "              f\"val_acc={logs['val_accuracy']:.4f}\")\n",
    "\n",
    "def clone_and_compile(model, learning_rate=LEARNING_RATE):\n",
    "    m = tf.keras.models.clone_model(model)\n",
    "    m.set_weights(model.get_weights())\n",
    "    m.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return m\n",
    "\n",
    "# ================\n",
    "# Pruning helpers\n",
    "# ================\n",
    "def manual_prune(model, threshold=0.1):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            w[np.abs(w) < threshold] = 0\n",
    "            layer.set_weights([w, b])\n",
    "    return model\n",
    "\n",
    "def global_prune(model, sparsity=0.7):\n",
    "    # Gather all conv/dense weights\n",
    "    all_w = []\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, _ = layer.get_weights()\n",
    "            all_w.append(np.abs(w).ravel())\n",
    "    if not all_w:\n",
    "        return model\n",
    "    all_w = np.concatenate(all_w)\n",
    "    thr = np.percentile(all_w, sparsity * 100.0)\n",
    "    # Apply\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            w[np.abs(w) < thr] = 0\n",
    "            layer.set_weights([w, b])\n",
    "    return model\n",
    "\n",
    "def layerwise_prune(model, sparsity=0.7):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            thr = np.percentile(np.abs(w), sparsity * 100.0)\n",
    "            w[np.abs(w) < thr] = 0\n",
    "            layer.set_weights([w, b])\n",
    "    return model\n",
    "\n",
    "def count_zero_weights(model):\n",
    "    total = 0\n",
    "    zero = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            total += w.size + b.size\n",
    "            zero  += np.sum(w == 0) + np.sum(b == 0)\n",
    "    return zero, total\n",
    "\n",
    "def measure_inference_time(model, dataset):\n",
    "    start = time.time()\n",
    "    _ = model.predict(dataset, verbose=0)\n",
    "    return time.time() - start\n",
    "\n",
    "def evaluate_model(model, name, path=None):\n",
    "    zero, total = count_zero_weights(model)\n",
    "    inf_time = measure_inference_time(model, test_ds)\n",
    "    _, acc = model.evaluate(test_ds, verbose=0)\n",
    "    if path:\n",
    "        model.save(path)\n",
    "    print(f\"📦 {name}: zeros={zero}/{total} ({(zero/total)*100:.2f}%), \"\n",
    "          f\"inference={inf_time:.4f}s, accuracy={acc:.4f}\")\n",
    "    return {\n",
    "        \"Method\": name,\n",
    "        \"% Zero Weights\": (zero/total)*100 if total > 0 else 0.0,\n",
    "        \"Test Accuracy\": acc,\n",
    "        \"Inference Time (s)\": inf_time,\n",
    "        \"Path\": path or \"\"\n",
    "    }\n",
    "\n",
    "# ================\n",
    "# Train baseline\n",
    "# ================\n",
    "print(\"\\n🚀 Training baseline CNN...\")\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "base_model = build_cnn(input_shape, num_classes, learning_rate=LEARNING_RATE)\n",
    "base_model.fit(train_ds, validation_data=val_ds, epochs=BASE_EPOCHS,\n",
    "               callbacks=[TrainingProgress()], verbose=0)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Baseline (unpruned)\n",
    "unpruned_path = os.path.join(SAVE_DIR, \"cnn_unpruned.h5\")\n",
    "results.append(evaluate_model(base_model, \"Unpruned\", path=unpruned_path))\n",
    "\n",
    "# ================\n",
    "# Manual pruning (fixed thresholds)\n",
    "# ================\n",
    "for t in MANUAL_THRESHOLDS:\n",
    "    print(f\"\\n✂️ Manual pruning at threshold={t} ...\")\n",
    "    m = clone_and_compile(base_model, learning_rate=LEARNING_RATE)\n",
    "    m = manual_prune(m, threshold=t)\n",
    "    if PRUNE_FINETUNE_EPOCHS > 0:\n",
    "        m.fit(train_ds, validation_data=val_ds, epochs=PRUNE_FINETUNE_EPOCHS,\n",
    "              callbacks=[TrainingProgress()], verbose=0)\n",
    "    path = os.path.join(SAVE_DIR, f\"cnn_pruned_manual_{t}.h5\")\n",
    "    results.append(evaluate_model(m, f\"Manual threshold {t}\", path=path))\n",
    "\n",
    "# ================\n",
    "# Global 70% pruning\n",
    "# ================\n",
    "print(\"\\n🌐 Global pruning at 70% ...\")\n",
    "g = clone_and_compile(base_model, learning_rate=LEARNING_RATE)\n",
    "g = global_prune(g, sparsity=GLOBAL_SPARSITY)\n",
    "if PRUNE_FINETUNE_EPOCHS > 0:\n",
    "    g.fit(train_ds, validation_data=val_ds, epochs=PRUNE_FINETUNE_EPOCHS,\n",
    "          callbacks=[TrainingProgress()], verbose=0)\n",
    "g_path = os.path.join(SAVE_DIR, \"cnn_pruned_global_0.7.h5\")\n",
    "results.append(evaluate_model(g, \"Global 70%\", path=g_path))\n",
    "\n",
    "# ================\n",
    "# Layer-wise 70% pruning\n",
    "# ================\n",
    "print(\"\\n🧱 Layer-wise pruning at 70% ...\")\n",
    "l = clone_and_compile(base_model, learning_rate=LEARNING_RATE)\n",
    "l = layerwise_prune(l, sparsity=LAYERWISE_SPARSITY)\n",
    "if PRUNE_FINETUNE_EPOCHS > 0:\n",
    "    l.fit(train_ds, validation_data=val_ds, epochs=PRUNE_FINETUNE_EPOCHS,\n",
    "          callbacks=[TrainingProgress()], verbose=0)\n",
    "l_path = os.path.join(SAVE_DIR, \"cnn_pruned_layerwise_0.7.h5\")\n",
    "results.append(evaluate_model(l, \"Layer-wise 70%\", path=l_path))\n",
    "\n",
    "# ================\n",
    "# Summary\n",
    "# ================\n",
    "df = pd.DataFrame(results, columns=[\"Method\", \"% Zero Weights\", \"Test Accuracy\", \"Inference Time (s)\", \"Path\"])\n",
    "print(\"\\n✅ Pruning Comparison Summary:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Save summary to CSV for reference\n",
    "summary_csv = os.path.join(SAVE_DIR, \"pruning_summary.csv\")\n",
    "df.to_csv(summary_csv, index=False)\n",
    "print(f\"\\n📝 Summary saved to {summary_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14012885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training baseline CNN...\n",
      "✅ Epoch 1: loss=1.0434, val_loss=0.9344, acc=0.6699, val_acc=0.6690\n",
      "✅ Epoch 2: loss=0.9522, val_loss=0.8726, acc=0.6679, val_acc=0.6680\n",
      "✅ Epoch 3: loss=0.9224, val_loss=0.9168, acc=0.6746, val_acc=0.6690\n",
      "✅ Epoch 4: loss=0.8825, val_loss=0.8445, acc=0.6846, val_acc=0.6919\n",
      "✅ Epoch 5: loss=0.8561, val_loss=0.8558, acc=0.6917, val_acc=0.6889\n",
      "✅ Epoch 6: loss=0.8385, val_loss=0.8166, acc=0.6977, val_acc=0.6959\n",
      "✅ Epoch 7: loss=0.8203, val_loss=0.8196, acc=0.7032, val_acc=0.6979\n",
      "✅ Epoch 8: loss=0.8086, val_loss=0.7815, acc=0.7087, val_acc=0.7069\n",
      "✅ Epoch 9: loss=0.7964, val_loss=0.7699, acc=0.7053, val_acc=0.7079\n",
      "✅ Epoch 10: loss=0.7732, val_loss=0.7742, acc=0.7137, val_acc=0.7069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Unpruned: zeros=1/684103 (0.00%), Nonzero=684102, FLOPs=30.03M, Mem=2.61MB, PowerProxy=30.03M, inference=1.5896s, accuracy=0.7117\n",
      "\n",
      "✂️ Manual pruning at threshold=0.001 ...\n",
      "✅ Epoch 1: loss=0.7639, val_loss=0.7743, acc=0.7193, val_acc=0.7089\n",
      "✅ Epoch 2: loss=0.7492, val_loss=0.7506, acc=0.7254, val_acc=0.7129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Manual threshold 0.001: zeros=11708/684103 (1.71%), Nonzero=672395, FLOPs=30.03M, Mem=2.61MB, PowerProxy=29.52M, inference=1.5765s, accuracy=0.7207\n",
      "\n",
      "✂️ Manual pruning at threshold=0.01 ...\n",
      "✅ Epoch 1: loss=0.7616, val_loss=0.7520, acc=0.7174, val_acc=0.7079\n",
      "✅ Epoch 2: loss=0.7506, val_loss=0.7599, acc=0.7160, val_acc=0.6999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Manual threshold 0.01: zeros=117007/684103 (17.10%), Nonzero=567096, FLOPs=30.03M, Mem=2.61MB, PowerProxy=24.89M, inference=1.5929s, accuracy=0.7042\n",
      "\n",
      "✂️ Manual pruning at threshold=0.05 ...\n",
      "✅ Epoch 1: loss=0.7779, val_loss=0.7541, acc=0.7117, val_acc=0.7258\n",
      "✅ Epoch 2: loss=0.7511, val_loss=0.7662, acc=0.7206, val_acc=0.7149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Manual threshold 0.05: zeros=450877/684103 (65.91%), Nonzero=233226, FLOPs=30.03M, Mem=2.61MB, PowerProxy=10.24M, inference=1.8549s, accuracy=0.7167\n",
      "\n",
      "✂️ Manual pruning at threshold=0.1 ...\n",
      "✅ Epoch 1: loss=0.9025, val_loss=0.7820, acc=0.6846, val_acc=0.7069\n",
      "✅ Epoch 2: loss=0.8102, val_loss=0.7691, acc=0.7070, val_acc=0.7029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Manual threshold 0.1: zeros=490744/684103 (71.74%), Nonzero=193359, FLOPs=30.03M, Mem=2.61MB, PowerProxy=8.49M, inference=1.4958s, accuracy=0.7112\n",
      "\n",
      "🌐 Global pruning at 70% ...\n",
      "✅ Epoch 1: loss=0.7698, val_loss=0.7645, acc=0.7157, val_acc=0.7159\n",
      "✅ Epoch 2: loss=0.7534, val_loss=0.7516, acc=0.7191, val_acc=0.7119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Global 70%: zeros=361484/684103 (52.84%), Nonzero=322619, FLOPs=30.03M, Mem=2.61MB, PowerProxy=14.16M, inference=1.5370s, accuracy=0.7182\n",
      "\n",
      "🧱 Layer-wise pruning at 70% ...\n",
      "✅ Epoch 1: loss=0.8068, val_loss=0.7652, acc=0.7049, val_acc=0.7049\n",
      "✅ Epoch 2: loss=0.7700, val_loss=0.7590, acc=0.7151, val_acc=0.7069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Layer-wise 70%: zeros=360243/684103 (52.66%), Nonzero=323860, FLOPs=30.03M, Mem=2.61MB, PowerProxy=14.22M, inference=1.6484s, accuracy=0.7102\n",
      "\n",
      "✅ Pruning Comparison Summary:\n",
      "                Method  % Zero Weights  Nonzero Weights  FLOPs (M)  Memory (MB)  Power Proxy (M)  Test Accuracy  Inference Time (s)                                     Path\n",
      "              Unpruned        0.000146           684102   30.03008     2.609646        30.030036       0.711721            1.589569             saved_models/cnn_unpruned.h5\n",
      "Manual threshold 0.001        1.711438           672395   30.03008     2.609646        29.516134       0.720698            1.576545  saved_models/cnn_pruned_manual_0.001.h5\n",
      " Manual threshold 0.01       17.103711           567096   30.03008     2.609646        24.893822       0.704239            1.592899   saved_models/cnn_pruned_manual_0.01.h5\n",
      " Manual threshold 0.05       65.907765           233226   30.03008     2.609646        10.237925       0.716708            1.854890   saved_models/cnn_pruned_manual_0.05.h5\n",
      "  Manual threshold 0.1       71.735397           193359   30.03008     2.609646         8.487883       0.711222            1.495832    saved_models/cnn_pruned_manual_0.1.h5\n",
      "            Global 70%       52.840581           322619   30.03008     2.609646        14.162011       0.718204            1.536999    saved_models/cnn_pruned_global_0.7.h5\n",
      "        Layer-wise 70%       52.659176           323860   30.03008     2.609646        14.216487       0.710224            1.648445 saved_models/cnn_pruned_layerwise_0.7.h5\n",
      "\n",
      "📝 Summary saved to saved_models/pruning_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# ================\n",
    "# Config\n",
    "# ================\n",
    "FILE_PATH = \"/Users/arihangupta/.medmnist/dermamnist.npz\"\n",
    "SAVE_DIR = \"saved_models\"\n",
    "BASE_EPOCHS = 10\n",
    "PRUNE_FINETUNE_EPOCHS = 2\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "IMG_SIZE = 64\n",
    "MANUAL_THRESHOLDS = [0.001, 0.01, 0.05, 0.1]\n",
    "GLOBAL_SPARSITY = 0.7   # 70%\n",
    "LAYERWISE_SPARSITY = 0.7\n",
    "\n",
    "# Optional: reproducibility\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ================\n",
    "# Data\n",
    "# ================\n",
    "data = np.load(FILE_PATH)\n",
    "X_train, y_train = data[\"train_images\"], data[\"train_labels\"].flatten()\n",
    "X_val,   y_val   = data[\"val_images\"],   data[\"val_labels\"].flatten()\n",
    "X_test,  y_test  = data[\"test_images\"],  data[\"test_labels\"].flatten()\n",
    "\n",
    "# Normalize\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_val   = X_val.astype(\"float32\") / 255.0\n",
    "X_test  = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Ensure 3 channels\n",
    "def to_3ch(arr):\n",
    "    if arr.ndim == 3 or arr.shape[-1] == 1:\n",
    "        return np.repeat(arr[..., np.newaxis], 3, axis=-1)\n",
    "    return arr\n",
    "X_train, X_val, X_test = to_3ch(X_train), to_3ch(X_val), to_3ch(X_test)\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# tf.data pipelines with resize\n",
    "def make_dataset(images, labels, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(images))\n",
    "    ds = ds.map(lambda x, y: (tf.image.resize(x, [IMG_SIZE, IMG_SIZE]), y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_dataset(X_train, y_train, shuffle=True)\n",
    "val_ds   = make_dataset(X_val, y_val, shuffle=False)\n",
    "test_ds  = make_dataset(X_test, y_test, shuffle=False)\n",
    "\n",
    "# ================\n",
    "# Model\n",
    "# ================\n",
    "def build_cnn(input_shape, num_classes, learning_rate=LEARNING_RATE):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Conv2D(64, (3,3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Conv2D(128, (3,3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "class TrainingProgress(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"✅ Epoch {epoch+1}: \"\n",
    "              f\"loss={logs['loss']:.4f}, \"\n",
    "              f\"val_loss={logs['val_loss']:.4f}, \"\n",
    "              f\"acc={logs['accuracy']:.4f}, \"\n",
    "              f\"val_acc={logs['val_accuracy']:.4f}\")\n",
    "\n",
    "def clone_and_compile(model, learning_rate=LEARNING_RATE):\n",
    "    m = tf.keras.models.clone_model(model)\n",
    "    m.set_weights(model.get_weights())\n",
    "    m.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return m\n",
    "\n",
    "# ================\n",
    "# Pruning helpers\n",
    "# ================\n",
    "def manual_prune(model, threshold=0.1):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            w[np.abs(w) < threshold] = 0\n",
    "            layer.set_weights([w, b])\n",
    "    return model\n",
    "\n",
    "def global_prune(model, sparsity=0.7):\n",
    "    # Gather all conv/dense weights\n",
    "    all_w = []\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, _ = layer.get_weights()\n",
    "            all_w.append(np.abs(w).ravel())\n",
    "    if not all_w:\n",
    "        return model\n",
    "    all_w = np.concatenate(all_w)\n",
    "    thr = np.percentile(all_w, sparsity * 100.0)\n",
    "    # Apply\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            w[np.abs(w) < thr] = 0\n",
    "            layer.set_weights([w, b])\n",
    "    return model\n",
    "\n",
    "def layerwise_prune(model, sparsity=0.7):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            thr = np.percentile(np.abs(w), sparsity * 100.0)\n",
    "            w[np.abs(w) < thr] = 0\n",
    "            layer.set_weights([w, b])\n",
    "    return model\n",
    "\n",
    "def count_zero_weights(model):\n",
    "    total = 0\n",
    "    zero = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            w, b = layer.get_weights()\n",
    "            total += w.size + b.size\n",
    "            zero  += np.sum(w == 0) + np.sum(b == 0)\n",
    "    return zero, total\n",
    "\n",
    "# ================\n",
    "# Extra metrics: FLOPs & Memory\n",
    "# ================\n",
    "def compute_flops_and_memory(model, input_shape=(64, 64, 3)):\n",
    "    # Ensure model is built\n",
    "    if not model.built:\n",
    "        model.build((None,) + input_shape)\n",
    "\n",
    "    # Run a dummy forward pass to make sure layer.input/output shapes exist\n",
    "    dummy_input = tf.random.normal((1,) + input_shape)\n",
    "    _ = model(dummy_input, training=False)\n",
    "\n",
    "    flops = 0\n",
    "    params = model.count_params()\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            out_shape = layer.output.shape\n",
    "            h, w = int(out_shape[1]), int(out_shape[2])\n",
    "            kh, kw = layer.kernel_size\n",
    "            in_ch = int(layer.input.shape[-1])\n",
    "            out_ch = layer.filters\n",
    "            flops += kh * kw * in_ch * out_ch * h * w\n",
    "\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            in_units = int(layer.input.shape[-1])\n",
    "            out_units = layer.units\n",
    "            flops += in_units * out_units\n",
    "\n",
    "    memory_mb = params * 4 / (1024**2)  # 4 bytes per float32\n",
    "    return flops, memory_mb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def measure_inference_time(model, dataset):\n",
    "    start = time.time()\n",
    "    _ = model.predict(dataset, verbose=0)\n",
    "    return time.time() - start\n",
    "\n",
    "def evaluate_model(model, name, path=None):\n",
    "    zero, total = count_zero_weights(model)\n",
    "    inf_time = measure_inference_time(model, test_ds)\n",
    "    _, acc = model.evaluate(test_ds, verbose=0)\n",
    "    flops, memory_mb = compute_flops_and_memory(model)\n",
    "\n",
    "    # Proxy for power = FLOPs scaled by nonzero fraction\n",
    "    nonzero_frac = (total - zero) / total if total > 0 else 1\n",
    "    power_proxy = flops * nonzero_frac\n",
    "\n",
    "    if path:\n",
    "        model.save(path)\n",
    "\n",
    "    print(f\"📦 {name}: zeros={zero}/{total} ({(zero/total)*100:.2f}%), \"\n",
    "          f\"Nonzero={total-zero}, FLOPs={flops/1e6:.2f}M, \"\n",
    "          f\"Mem={memory_mb:.2f}MB, PowerProxy={power_proxy/1e6:.2f}M, \"\n",
    "          f\"inference={inf_time:.4f}s, accuracy={acc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"Method\": name,\n",
    "        \"% Zero Weights\": (zero/total)*100 if total > 0 else 0.0,\n",
    "        \"Nonzero Weights\": total - zero,\n",
    "        \"FLOPs (M)\": flops/1e6,\n",
    "        \"Memory (MB)\": memory_mb,\n",
    "        \"Power Proxy (M)\": power_proxy/1e6,\n",
    "        \"Test Accuracy\": acc,\n",
    "        \"Inference Time (s)\": inf_time,\n",
    "        \"Path\": path or \"\"\n",
    "    }\n",
    "\n",
    "# ================\n",
    "# Train baseline\n",
    "# ================\n",
    "print(\"\\n🚀 Training baseline CNN...\")\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "base_model = build_cnn(input_shape, num_classes, learning_rate=LEARNING_RATE)\n",
    "base_model.fit(train_ds, validation_data=val_ds, epochs=BASE_EPOCHS,\n",
    "               callbacks=[TrainingProgress()], verbose=0)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Baseline (unpruned)\n",
    "unpruned_path = os.path.join(SAVE_DIR, \"cnn_unpruned.h5\")\n",
    "results.append(evaluate_model(base_model, \"Unpruned\", path=unpruned_path))\n",
    "\n",
    "# ================\n",
    "# Manual pruning (fixed thresholds)\n",
    "# ================\n",
    "for t in MANUAL_THRESHOLDS:\n",
    "    print(f\"\\n✂️ Manual pruning at threshold={t} ...\")\n",
    "    m = clone_and_compile(base_model, learning_rate=LEARNING_RATE)\n",
    "    m = manual_prune(m, threshold=t)\n",
    "    if PRUNE_FINETUNE_EPOCHS > 0:\n",
    "        m.fit(train_ds, validation_data=val_ds, epochs=PRUNE_FINETUNE_EPOCHS,\n",
    "              callbacks=[TrainingProgress()], verbose=0)\n",
    "    path = os.path.join(SAVE_DIR, f\"cnn_pruned_manual_{t}.h5\")\n",
    "    results.append(evaluate_model(m, f\"Manual threshold {t}\", path=path))\n",
    "\n",
    "# ================\n",
    "# Global 70% pruning\n",
    "# ================\n",
    "print(\"\\n🌐 Global pruning at 70% ...\")\n",
    "g = clone_and_compile(base_model, learning_rate=LEARNING_RATE)\n",
    "g = global_prune(g, sparsity=GLOBAL_SPARSITY)\n",
    "if PRUNE_FINETUNE_EPOCHS > 0:\n",
    "    g.fit(train_ds, validation_data=val_ds, epochs=PRUNE_FINETUNE_EPOCHS,\n",
    "          callbacks=[TrainingProgress()], verbose=0)\n",
    "g_path = os.path.join(SAVE_DIR, \"cnn_pruned_global_0.7.h5\")\n",
    "results.append(evaluate_model(g, \"Global 70%\", path=g_path))\n",
    "\n",
    "# ================\n",
    "# Layer-wise 70% pruning\n",
    "# ================\n",
    "print(\"\\n🧱 Layer-wise pruning at 70% ...\")\n",
    "l = clone_and_compile(base_model, learning_rate=LEARNING_RATE)\n",
    "l = layerwise_prune(l, sparsity=LAYERWISE_SPARSITY)\n",
    "if PRUNE_FINETUNE_EPOCHS > 0:\n",
    "    l.fit(train_ds, validation_data=val_ds, epochs=PRUNE_FINETUNE_EPOCHS,\n",
    "          callbacks=[TrainingProgress()], verbose=0)\n",
    "l_path = os.path.join(SAVE_DIR, \"cnn_pruned_layerwise_0.7.h5\")\n",
    "results.append(evaluate_model(l, \"Layer-wise 70%\", path=l_path))\n",
    "\n",
    "# ================\n",
    "# Summary\n",
    "# ================\n",
    "df = pd.DataFrame(results, columns=[\n",
    "    \"Method\", \"% Zero Weights\", \"Nonzero Weights\",\n",
    "    \"FLOPs (M)\", \"Memory (MB)\", \"Power Proxy (M)\",\n",
    "    \"Test Accuracy\", \"Inference Time (s)\", \"Path\"\n",
    "])\n",
    "print(\"\\n✅ Pruning Comparison Summary:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Save summary to CSV for reference\n",
    "summary_csv = os.path.join(SAVE_DIR, \"pruning_summary.csv\")\n",
    "df.to_csv(summary_csv, index=False)\n",
    "print(f\"\\n📝 Summary saved to {summary_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
